{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/update-notebooks/notebooks/colab-huggingface-API/Train_GPT_for_Conversation_w_Huggingface_and_Deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "959PjU1QMUbB"
      },
      "source": [
        "# <center> Train GPT for Conversation w Huggingface and Deepspeed </center>\n",
        "\n",
        "> purpose: train a GPT generation model that can be used for conversation using the huggingface `trainer` API\n",
        "\n",
        "material in this notebook is covered in:\n",
        "- [fine-tune on your own data](https://huggingface.co/docs/transformers/training)\n",
        "- [train bigger models faster](https://huggingface.co/docs/transformers/performance) also by huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XkAJwJkyjN-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UQWhgZ22b1h",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4a6738-9ef2-45ff-a292-75a7554ef0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime has 51.0 gigs of memory and 8 processors\n"
          ]
        }
      ],
      "source": [
        "#@title check  system stats\n",
        "from psutil import virtual_memory\n",
        "import os\n",
        "ram_gb = round(virtual_memory().total / (1024**3), 1)\n",
        "print(f'Runtime has {ram_gb} gigs of memory and {os.cpu_count()} processors')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH0JtdCeE1n2",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0669fad4-e076-4cd9-ef67-e21f9eccafc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 15 19:17:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title check GPU stats\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35wDBYrZyj5s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title set up auto-formatting of cells in notebook\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7_e3GBMPX_M",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "141dde11-df0f-41cf-8701-6c6a31a30006"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title install packages \n",
        "#@markdown git-lfs for saving, transformers and deepspeed\n",
        "!pip install transformers[fairscale] -U -q\n",
        "!pip install deepspeed -q\n",
        "!sudo apt-get install git-lfs -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmOJXkBRcMBX",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e2a4028f-506e-4e59-9521-b4e747795a94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed all checkpoints in /content/checkpoints\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import shutil, os, gc\n",
        "\n",
        "#@markdown clear out existing checkpoints (if restarting runtime)\n",
        "chkpt_path = \"/content/checkpoints\"\n",
        "if os.path.exists(chkpt_path): \n",
        "    shutil.rmtree(chkpt_path, True)\n",
        "    print(f\"removed all checkpoints in {chkpt_path}\")    \n",
        "\n",
        "fin_path = \"/content/final_model\"\n",
        "if os.path.exists(fin_path): \n",
        "    shutil.rmtree(fin_path, True)\n",
        "    print(f\"removed all final models in {fin_path}\")    \n",
        "\n",
        "fin_zero_path = \"/content/final_zero_weights\"\n",
        "if os.path.exists(fin_zero_path): \n",
        "    shutil.rmtree(fin_zero_path, True)\n",
        "    print(f\"removed the zero weights folder in {fin_zero_path}\")    \n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__m_tu2pD3Z"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498,
          "referenced_widgets": [
            "899e7d3891714a37a3ebf0ac9ec2c858",
            "f0c3ea78865640a19c02fd90abbb2708",
            "85c783f6586b4cc8b4ddb956170d1fdb",
            "9eda01fe0d9c484289a77046f2bfd493",
            "18dbb4e3ec7240098919668761638d4c",
            "c70cb772f1b642868905d91e956130ab",
            "81e719334cf54a55bd7aab0e421203f5",
            "eb63a170abfb457e8bd1936e3fc67bc3",
            "098edc6c54a5451590d7f7f6676201b6",
            "0465084b271a42859d1ea7d12b134286",
            "2738743dea684917be3d422d770bb555",
            "12c0116fe884437282939db409d618d8",
            "5de0826cbb2940568e6d0292a8ce10ed",
            "7a3034f119814069a9b9e015c6d4681d",
            "abf6ece647974d399640fe97aeac08cc",
            "5765b7bc7c114f2983258b8be4d3839d",
            "93f74d7a2abd4276b60923a961d6a793"
          ]
        },
        "outputId": "22795d98-4aed-4595-9931-af8afe264e7c",
        "id": "RGC-J8c-oxjA"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "899e7d3891714a37a3ebf0ac9ec2c858",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title <font color=\"orange\"> Sign in to HF </font>\n",
        "#@markdown create an account on their website if zou don't have one - you need somewhere to put this.\n",
        "\n",
        "#@markdown also imports a lot of the functions from the package\n",
        "from huggingface_hub import (\n",
        "    # User management\n",
        "    login,\n",
        "    logout,\n",
        "    notebook_login,\n",
        "    whoami,\n",
        "    # Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility,\n",
        "    # And some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AfCZ6IFBmBa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7784abb7-a676-4a63-8a13-66386ca7340d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown **Enter the huggingface model ID to use as a starting point:**\n",
        "\n",
        "#@markdown generic starting points would be `EleutherAI/gpt-neo-2.7B` or `EleutherAI/gpt-neo-1.3B`\n",
        "hf_name = \"pszemraj/GPT-Converse-1pt3B-Neo-WoW-DD-17\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMzgWbKyaKBL",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "07baabbd-9c07-433f-faf1-236561a8de7e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "N_EPOCHS =  10#@param {type:\"number\"}\n",
        "BATCH_SIZE =  32#@param {type:\"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ibmfs-45yqQ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a6b55e9d-f973-4219-a0d9-e4fef793e87b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name_header = \"C2\" #@param {type:\"string\"}\n",
        "dataset = \"WeW\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown the model name is ...\n",
        "full_out_name = f\"{hf_name.split('/')[-1]}-{model_name_header}_DS-{dataset}_Ep-{N_EPOCHS}_Bs-{BATCH_SIZE}\"\n",
        "# model.push_to_hub(full_out_name, auth)\n",
        "full_out_name = full_out_name.replace(\".\", \"pt\")\n",
        "print(f\"model will be saved on huggingface with the name:\\n\\t{full_out_name}\")\n",
        "print(\"note that this name can be changed in the model card later\")"
      ],
      "metadata": {
        "id": "8jCduzZwKqG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "cellView": "form",
        "outputId": "cdc3729d-1c44-4577-bd6c-141b702f9aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model will be saved on huggingface with the name:\n",
            "\tGPT-Converse-1pt3B-Neo-WoW-DD-17-C2_DS-WeW_Ep-10_Bs-32\n",
            "note that this name can be changed in the model card later\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lan1_NEOa46Y"
      },
      "source": [
        "### ideas for future setups\n",
        "\n",
        "\n",
        "1. try doing tokenization by method. I.e. we have person alpha, person beta, and _similar to the netflix example_ we split starting with custom tokens from one speaker to the other\n",
        "\n",
        "---\n",
        "\n",
        "- article [on padding](https://huggingface.co/docs/transformers/preprocessing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGLFHxeqp1E3",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2e6696c-6d37-4b7b-b6d1-d231fc405140"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f789ee28ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#@title import basic packages\n",
        "import os\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPTNeoForCausalLM\n",
        "from tqdm.auto import tqdm \n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "z_ljPO1cmcM6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHvkV1YHpG9z"
      },
      "source": [
        "## set up DeepSpeed config\n",
        "\n",
        "- As recommended [here](https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-example) from hf\n",
        "- the article above has recommendations on what to change for either improving runtime or adjustments to make less powerful GPUs work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF7c3Dn-YNUn"
      },
      "source": [
        "As recommended [here](https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-example) from hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH9DQ2lE1njE",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "91b7457b-a1b9-449c-f559-9970cd14c100"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%bash\n",
        "#@markdown **zero2 config.**\n",
        "\n",
        "#@markdown file will be saved under ds_config_zero2.json\n",
        "cat <<'EOT' > ds_config_zero2.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"allgather_partitions\": true,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"reduce_scatter\": true,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9ummCxgwe_A",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "957e6aa5-45d1-4a14-91e2-7e81a50aab74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%bash\n",
        "#@markdown **zero3 config - this is slower but uses CPU**\n",
        "\n",
        "#@markdown file will be saved under ds_config_zero3.json\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MKW3zKOZ24N",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2138ecce-2b12-4ceb-8ef3-e79c306926e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown create logging folder\n",
        "from os.path import join\n",
        "os.makedirs(join(os.getcwd(), \"logs\"), exist_ok=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH1W6YrdjNSp"
      },
      "source": [
        "# Prepare the dataset and build a ``TextDataset``\n",
        "\n",
        "The next step is to extract the instructions from all recipes and build a `TextDataset`. The `TextDataset` is a custom implementation of the [Pytroch `Dataset` class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) implemented by the transformers library. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd65U4bMmUBf"
      },
      "source": [
        "may need to adjust [how the dataset is split](https://huggingface.co/docs/datasets/splits.html) later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V36gOIOfLHvB",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1c253132-198f-4be1-c92b-1a9df629fad6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown import basics: `train_test_split` etc\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ZhzPjrPYlpj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "550b1c2f-a2fd-4379-e23e-0ef565d5cf30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_link = \"https://www.dropbox.com/s/olnx438omur7j72/wow-train.txt.txt?dl=1\" #@param {type:\"string\"}\n",
        "test_link = \"https://www.dropbox.com/s/t2hhawpsiocypyt/ScriptParse-wow-train-kilt_4.txt?dl=1\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCaCYtpRYin1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76c273bd-6d50-4b7b-f94d-03d15304bfb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/test_dataset.txt', <http.client.HTTPMessage at 0x7f799ff27610>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#@markdown download text dataset files\n",
        "\n",
        "vm_wd = os.getcwd()\n",
        "train_path = join(vm_wd, \"train_dataset.txt\")\n",
        "request.urlretrieve(train_link, train_path)\n",
        "\n",
        "# test file\n",
        "test_path = join(vm_wd, \"test_dataset.txt\")\n",
        "request.urlretrieve(test_link, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9lHS0mIMak4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198,
          "referenced_widgets": [
            "2a33ac2f15e44d7eaefd115832ee82fa",
            "9060157f56db472e8b758da2a3f7c833",
            "ba2f1e713ccf4d5da90943fb50a09927",
            "8167a4e596dd4920af60238db8d2412d",
            "f61eea0da924472897d21e4735ab3105",
            "d1641f7633c84088a7f670312deda1d2",
            "bc7136de1ada4c2a9dffe22536a1b631",
            "536004f38fbf407a94b8d6b1575aa194",
            "74676f70cf5a4458915f94e84ae07cc0",
            "fd4fb007e7624cea9f745e981a1e7b81",
            "43293ece0f6b482888e2aadff842bec6",
            "1af3a61b01964a448b2c066b1f4c99eb",
            "329488f885444705b1345415c82f583a",
            "5c29ff2da3cf44fb8da08c5be4226b04",
            "aba558da08124e1c98cd548a6bee4a5e",
            "8b8f7311056744ccb67952270fe32a37",
            "c14cf90d94ad47218bcd488ffab1480f",
            "953c7ba481e149dba88bb44dc066367e",
            "275133dc9e8648b8a6ec903630d09857",
            "7712fd32c66a48f18a03c8b0e52efe1f",
            "9f76fa85494245c0b1d415709d456c2e",
            "d21fe997e5ca426c80859d6f01c5869a",
            "948c35c31a2247bb990750728424ac0f",
            "766e587045434959bafaf580750bd62f",
            "c777053ed1b449bc87c5b7fc2ed9ff76",
            "dfc942da04e6408c95d61ae2211ae94a",
            "d07eba7c5c29421ba8266c0ed91f4eff",
            "3d57cf64827646b2ad0140e99370f0a9",
            "9b930058869849d0921c6180e13ebb6f",
            "d0f56714852040df9648d3a9147995d8",
            "7a2b85655e984e97aa6d46484c5c0e03",
            "c0fab74f6f9348e89cd077674e3fbede",
            "6a8141a298cb42079597e5f3be88b952",
            "a52cf3de0cec440c97775e75e9436eba",
            "a31dfc13d8ec44e5888e48ff7072a050",
            "41148e8d8cfb420b96dd548fc4a41c16",
            "f062e208f0f6498d907f6c2be43f53da",
            "b99402cdac3744ea86414f4ea29bf92b",
            "b17a627055744668a515a357572d86e4",
            "b33c02e7b6c14a2a8b83c1be70ada332",
            "c53f4c2fde134f4bbbe4ab8139918ef4",
            "9f030d8a1d87434997358fb2fd0a5443",
            "e4cf1972b56041c8a5bae16335bc5d26",
            "60c7bd3253974cfc8adcb594a0e9565b"
          ]
        },
        "outputId": "099d2cd1-b126-4f6d-cb71-f62c4569117b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a33ac2f15e44d7eaefd115832ee82fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/682 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1af3a61b01964a448b2c066b1f4c99eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "948c35c31a2247bb990750728424ac0f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a52cf3de0cec440c97775e75e9436eba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import AutoTokenizer\n",
        "#@markdown create helper function  for test/train data. if the current function \n",
        "#@markdown stops working (i.e. the TextDataset etc) just update it to [this script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py)\n",
        "#@markdown on the HF repo\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "     \n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "_tokenizer = AutoTokenizer.from_pretrained(hf_name)\n",
        "\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,\n",
        "                                                        test_path,\n",
        "                                                        _tokenizer)\n",
        "\n",
        "# note that are using the NEW tokenizer here (_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG5ZSa-CqeOS",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "095826aee8ff41f6b64ce8838006ee99",
            "d555ee7fc28b48d99b4e253a769d5139",
            "26fa7857940b400b8cacea3b8b772a40",
            "43a651e7b1ac4e0e885b4a7f1fdd6b61",
            "1432523c1da54e2ead897a2bc06e3624",
            "e72128ec36d6436789ef01c5b82f46f5",
            "08626a636f5545f89ff0e0ec84148638",
            "75a1bc7b111c467e9bf741aad1ce6c88",
            "236d79159e8648d79e9ccae6d2e79532",
            "2d3e252073d74e9e86837e6433f168b0",
            "7c70bb3d6dc14c6ca5f0572dc15d33a0",
            "68d87caed46c4e039cf518143eb746e0",
            "2a8e4f95ec884c18b092f0752869e762",
            "a39dab9b8d5044cc9bf0d9ecb142f485",
            "49a1e02324604622a5d5e92d3f9eb8be",
            "7ccbc92bec44416d832c5010023db41b",
            "0f9bf7c9114e44fabe0a5e3e6a456d2b",
            "7725fbdbdf244edc88f31c4f197cc614",
            "6c5b2d88a6ae418e98e3d6c5fd492580",
            "ba39b412860f414ba7b81dc79a752d83",
            "e44fbd9d29a94ecab8f7e8b4788b8725",
            "4c06da8044b84f17affd904515f2547f"
          ]
        },
        "outputId": "04d04aa0-008a-498d-c2d9-c832d1a6376e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "095826aee8ff41f6b64ce8838006ee99",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68d87caed46c4e039cf518143eb746e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 31s, sys: 20.6 s, total: 1min 51s\n",
            "Wall time: 3min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import AutoModelForCausalLM\n",
        "#@title load the text gen model \n",
        "#@markdown - downloads model file if needed\n",
        "#@markdown - resize the token embeddings as needed\n",
        "model = AutoModelForCausalLM.from_pretrained(hf_name, \n",
        "                                             use_cache=False,\n",
        "                                             low_cpu_mem_usage=True,\n",
        "                                        ).cuda()\n",
        "model.resize_token_embeddings(len(_tokenizer))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThG6jwL7qET8"
      },
      "source": [
        "# Initialize `Trainer` with `TrainingArguments` and GPT-2 model\n",
        "\n",
        "TheÂ [Trainer](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer)Â class provides an API for feature-complete training. It is used in most of theÂ [example scripts](https://huggingface.co/transformers/examples.html) from Huggingface. Before we can instantiate our `Trainer` we need to download our GPT-2 model and create aÂ [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)Â to access all the points of customization during training. In the `TrainingArguments`, we can define the Hyperparameters we are going to use in the training process like our `learning_rate`, `num_train_epochs`, or  `per_device_train_batch_size`. A complete list can you find [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYOrB3Ezzvag",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6bcb7ff0-3de8-4d84-a2ec-fab8bd7cee66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title create `os.environ`setups for the notebook\n",
        "# DeepSpeed requires a distributed environment even when only one process is used.\n",
        "# This emulates a launcher in the notebook\n",
        "import os\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"\n",
        "os.environ['WORLD_SIZE'] = \"1\"\n",
        "\n",
        "# # Now proceed as normal, plus pass the deepspeed config file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title choose deepspeed config\n",
        "#@markdown ZeR03 is slower, but is more GPU friendly. If just trying to get it to work, use that.\n",
        "# from filenames defined earlier\n",
        "ds_config = \"/content/ds_config_zero3.json\" #@param [\"/content/ds_config_zero3.json\", \"/content/ds_config_zero2.json\"] {allow-input: true}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BRMYiqWE5AK6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "28d40ccb-f207-4c31-a8cc-455e3bad7eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title configure hyperparameters - training\n",
        "GRAD_ACC_STEPS =  32#@param {type:\"integer\"}\n",
        "MAX_SAVES_ON_HD =  1#@param {type:\"integer\"}\n",
        "RATIO_WARMUP =  0.05#@param {type:\"number\"}\n",
        "WEIGHT_DECAY =  0.1#@param {type:\"number\"}\n",
        "LEARNING_RATE =  7e-5#@param {type:\"number\"}\n",
        "LR_SCHEDULER =  \"constant_with_warmup\"#@param {type:\"string\"}\n",
        "MAX_GRADIENT_NORM =  0.5#@param {type:\"number\"}\n",
        "USE_HUB =  True #@param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "euRFFcaJhwqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0561dcf7-ee82-430a-cb7d-c7b59a23392b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7hhmbT2ModI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "cellView": "form",
        "outputId": "7bd877fb-5215-4764-c818-d12d42a8b47f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 19:21:03,674] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning https://huggingface.co/pszemraj/GPT-Converse-1pt3B-Neo-WoW-DD-17-C2_DS-WeW_Ep-10_Bs-32 into local empty directory.\n",
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "#@title create the `trainer`\n",
        "#@markdown create the `trainer` object using `TrainingArguments()`\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    output_dir='./checkpoints', \n",
        "    save_total_limit=MAX_SAVES_ON_HD,\n",
        "    logging_dir='/content/logs',\n",
        "    num_train_epochs=N_EPOCHS, \n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    per_device_train_batch_size=BATCH_SIZE, \n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    # eval_steps = 500, # Number of update steps between two evaluations.\n",
        "    # save_steps=100, # after # steps model is saved \n",
        "    gradient_accumulation_steps=GRAD_ACC_STEPS, # working A100 + 2.7B was 32\n",
        "    eval_accumulation_steps=max(int(GRAD_ACC_STEPS/2), 1),\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=MAX_GRADIENT_NORM,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lr_scheduler_type = LR_SCHEDULER,\n",
        "    warmup_ratio=RATIO_WARMUP,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    # bf16=True, \n",
        "    # bf16_full_eval=True,\n",
        "    fp16_full_eval=True,\n",
        "    fp16=True,\n",
        "    fp16_opt_level='O1',\n",
        "    deepspeed=ds_config, # use deepspeed.\n",
        "    push_to_hub=USE_HUB,\n",
        "    hub_model_id=full_out_name if USE_HUB else None,\n",
        "    hub_strategy='checkpoint' if USE_HUB else None,\n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=test_dataset, \n",
        "                  data_collator=data_collator,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyQh0jlTqR6-"
      },
      "source": [
        "# Train GPT Model\n",
        "\n",
        "API is simple: `Trainer.train()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZKTwIv2_V_G"
      },
      "source": [
        "## working configs\n",
        "\n",
        "```\n",
        "\n",
        "A100\n",
        "\n",
        "2.7 B\n",
        "- zero2\n",
        "- batch size 8\n",
        "\n",
        "1.3 B \n",
        "- zero2\n",
        "- batch size 16\n",
        "\n",
        "V100\n",
        "\n",
        "1.3 B\n",
        "= zero3\n",
        "- batch size 8\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjIzSWPTKzBf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "467c3e42-0e36-4ab1-93a5-4d2be2c2ba98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-01-15 19:21:12,018] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.9, git-hash=unknown, git-branch=unknown\n",
            "[2022-01-15 19:21:12,035] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups\n",
            "[2022-01-15 19:21:12,036] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n",
            "[2022-01-15 19:21:12,039] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n",
            "[2022-01-15 19:21:12,041] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
            "[2022-01-15 19:21:12,042] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
            "[2022-01-15 19:21:12,075] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "Using /root/.cache/torch_extensions/py37_cu111 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu111/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 3.389350414276123 seconds\n",
            "[2022-01-15 19:21:17,556] [INFO] [engine.py:1109:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2022-01-15 19:21:17,580] [INFO] [engine.py:1116:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2022-01-15 19:21:17,580] [INFO] [utils.py:44:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2022-01-15 19:21:17,581] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
            "[2022-01-15 19:21:17,582] [INFO] [engine.py:1384:_configure_zero_optimizer] Initializing ZeRO Stage 3\n",
            "[2022-01-15 19:21:17,590] [INFO] [stage3.py:639:__init__] Reduce bucket size 4194304\n",
            "[2022-01-15 19:21:17,591] [INFO] [stage3.py:640:__init__] Allgather bucket size 3774873.6\n",
            "Using /root/.cache/torch_extensions/py37_cu111 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu111/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.6980156898498535 seconds\n",
            "[2022-01-15 19:21:34,119] [INFO] [stage3.py:811:__init__] optimizer state initialized\n",
            "[2022-01-15 19:21:34,417] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2022-01-15 19:21:34,418] [INFO] [engine.py:798:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2022-01-15 19:21:34,419] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f7617bb4a50>\n",
            "[2022-01-15 19:21:34,420] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[7e-05], mom=[[0.9, 0.999]]\n",
            "[2022-01-15 19:21:34,422] [INFO] [config.py:1058:print] DeepSpeedEngine configuration:\n",
            "[2022-01-15 19:21:34,424] [INFO] [config.py:1062:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2022-01-15 19:21:34,426] [INFO] [config.py:1062:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2022-01-15 19:21:34,427] [INFO] [config.py:1062:print]   amp_enabled .................. False\n",
            "[2022-01-15 19:21:34,428] [INFO] [config.py:1062:print]   amp_params ................... False\n",
            "[2022-01-15 19:21:34,429] [INFO] [config.py:1062:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": null, \n",
            "    \"exps_dir\": null, \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2022-01-15 19:21:34,430] [INFO] [config.py:1062:print]   bfloat16_enabled ............. False\n",
            "[2022-01-15 19:21:34,431] [INFO] [config.py:1062:print]   checkpoint_tag_validation_enabled  True\n",
            "[2022-01-15 19:21:34,433] [INFO] [config.py:1062:print]   checkpoint_tag_validation_fail  False\n",
            "[2022-01-15 19:21:34,434] [INFO] [config.py:1062:print]   communication_data_type ...... None\n",
            "[2022-01-15 19:21:34,435] [INFO] [config.py:1062:print]   curriculum_enabled ........... False\n",
            "[2022-01-15 19:21:34,436] [INFO] [config.py:1062:print]   curriculum_params ............ False\n",
            "[2022-01-15 19:21:34,437] [INFO] [config.py:1062:print]   dataloader_drop_last ......... False\n",
            "[2022-01-15 19:21:34,438] [INFO] [config.py:1062:print]   disable_allgather ............ False\n",
            "[2022-01-15 19:21:34,439] [INFO] [config.py:1062:print]   dump_state ................... False\n",
            "[2022-01-15 19:21:34,441] [INFO] [config.py:1062:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2022-01-15 19:21:34,442] [INFO] [config.py:1062:print]   eigenvalue_enabled ........... False\n",
            "[2022-01-15 19:21:34,443] [INFO] [config.py:1062:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2022-01-15 19:21:34,444] [INFO] [config.py:1062:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2022-01-15 19:21:34,445] [INFO] [config.py:1062:print]   eigenvalue_layer_num ......... 0\n",
            "[2022-01-15 19:21:34,447] [INFO] [config.py:1062:print]   eigenvalue_max_iter .......... 100\n",
            "[2022-01-15 19:21:34,448] [INFO] [config.py:1062:print]   eigenvalue_stability ......... 1e-06\n",
            "[2022-01-15 19:21:34,449] [INFO] [config.py:1062:print]   eigenvalue_tol ............... 0.01\n",
            "[2022-01-15 19:21:34,450] [INFO] [config.py:1062:print]   eigenvalue_verbose ........... False\n",
            "[2022-01-15 19:21:34,451] [INFO] [config.py:1062:print]   elasticity_enabled ........... False\n",
            "[2022-01-15 19:21:34,453] [INFO] [config.py:1062:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2022-01-15 19:21:34,454] [INFO] [config.py:1062:print]   fp16_enabled ................. True\n",
            "[2022-01-15 19:21:34,455] [INFO] [config.py:1062:print]   fp16_master_weights_and_gradients  False\n",
            "[2022-01-15 19:21:34,456] [INFO] [config.py:1062:print]   fp16_mixed_quantize .......... False\n",
            "[2022-01-15 19:21:34,457] [INFO] [config.py:1062:print]   global_rank .................. 0\n",
            "[2022-01-15 19:21:34,458] [INFO] [config.py:1062:print]   gradient_accumulation_steps .. 32\n",
            "[2022-01-15 19:21:34,458] [INFO] [config.py:1062:print]   gradient_clipping ............ 0.5\n",
            "[2022-01-15 19:21:34,460] [INFO] [config.py:1062:print]   gradient_predivide_factor .... 1.0\n",
            "[2022-01-15 19:21:34,460] [INFO] [config.py:1062:print]   initial_dynamic_scale ........ 65536\n",
            "[2022-01-15 19:21:34,461] [INFO] [config.py:1062:print]   loss_scale ................... 0\n",
            "[2022-01-15 19:21:34,462] [INFO] [config.py:1062:print]   memory_breakdown ............. False\n",
            "[2022-01-15 19:21:34,463] [INFO] [config.py:1062:print]   optimizer_legacy_fusion ...... False\n",
            "[2022-01-15 19:21:34,464] [INFO] [config.py:1062:print]   optimizer_name ............... adamw\n",
            "[2022-01-15 19:21:34,465] [INFO] [config.py:1062:print]   optimizer_params ............. {'lr': 7e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.1}\n",
            "[2022-01-15 19:21:34,466] [INFO] [config.py:1062:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2022-01-15 19:21:34,468] [INFO] [config.py:1062:print]   pld_enabled .................. False\n",
            "[2022-01-15 19:21:34,469] [INFO] [config.py:1062:print]   pld_params ................... False\n",
            "[2022-01-15 19:21:34,470] [INFO] [config.py:1062:print]   prescale_gradients ........... False\n",
            "[2022-01-15 19:21:34,471] [INFO] [config.py:1062:print]   quantize_change_rate ......... 0.001\n",
            "[2022-01-15 19:21:34,471] [INFO] [config.py:1062:print]   quantize_groups .............. 1\n",
            "[2022-01-15 19:21:34,472] [INFO] [config.py:1062:print]   quantize_offset .............. 1000\n",
            "[2022-01-15 19:21:34,473] [INFO] [config.py:1062:print]   quantize_period .............. 1000\n",
            "[2022-01-15 19:21:34,475] [INFO] [config.py:1062:print]   quantize_rounding ............ 0\n",
            "[2022-01-15 19:21:34,476] [INFO] [config.py:1062:print]   quantize_start_bits .......... 16\n",
            "[2022-01-15 19:21:34,477] [INFO] [config.py:1062:print]   quantize_target_bits ......... 8\n",
            "[2022-01-15 19:21:34,478] [INFO] [config.py:1062:print]   quantize_training_enabled .... False\n",
            "[2022-01-15 19:21:34,479] [INFO] [config.py:1062:print]   quantize_type ................ 0\n",
            "[2022-01-15 19:21:34,480] [INFO] [config.py:1062:print]   quantize_verbose ............. False\n",
            "[2022-01-15 19:21:34,481] [INFO] [config.py:1062:print]   scheduler_name ............... WarmupLR\n",
            "[2022-01-15 19:21:34,482] [INFO] [config.py:1062:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 7e-05, 'warmup_num_steps': 26}\n",
            "[2022-01-15 19:21:34,483] [INFO] [config.py:1062:print]   sparse_attention ............. None\n",
            "[2022-01-15 19:21:34,484] [INFO] [config.py:1062:print]   sparse_gradients_enabled ..... False\n",
            "[2022-01-15 19:21:34,484] [INFO] [config.py:1062:print]   steps_per_print .............. 2000\n",
            "[2022-01-15 19:21:34,485] [INFO] [config.py:1062:print]   tensorboard_enabled .......... False\n",
            "[2022-01-15 19:21:34,486] [INFO] [config.py:1062:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2022-01-15 19:21:34,487] [INFO] [config.py:1062:print]   tensorboard_output_path ...... \n",
            "[2022-01-15 19:21:34,488] [INFO] [config.py:1062:print]   train_batch_size ............. 1024\n",
            "[2022-01-15 19:21:34,488] [INFO] [config.py:1062:print]   train_micro_batch_size_per_gpu  32\n",
            "[2022-01-15 19:21:34,489] [INFO] [config.py:1062:print]   use_quantizer_kernel ......... False\n",
            "[2022-01-15 19:21:34,490] [INFO] [config.py:1062:print]   wall_clock_breakdown ......... False\n",
            "[2022-01-15 19:21:34,491] [INFO] [config.py:1062:print]   world_size ................... 1\n",
            "[2022-01-15 19:21:34,492] [INFO] [config.py:1062:print]   zero_allow_untested_optimizer  False\n",
            "[2022-01-15 19:21:34,494] [INFO] [config.py:1062:print]   zero_config .................. {\n",
            "    \"stage\": 3, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 4.194304e+06, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 5.000000e+08, \n",
            "    \"overlap_comm\": true, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": true, \n",
            "    \"offload_param\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 5, \n",
            "        \"buffer_size\": 1.000000e+08, \n",
            "        \"max_in_cpu\": 1.000000e+09, \n",
            "        \"pin_memory\": true\n",
            "    }, \n",
            "    \"offload_optimizer\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 4, \n",
            "        \"pin_memory\": true, \n",
            "        \"pipeline_read\": false, \n",
            "        \"pipeline_write\": false, \n",
            "        \"fast_init\": false, \n",
            "        \"pipeline\": false\n",
            "    }, \n",
            "    \"sub_group_size\": 1.000000e+09, \n",
            "    \"prefetch_bucket_size\": 3.774874e+06, \n",
            "    \"param_persistence_threshold\": 2.048000e+04, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_fp16_weights_on_model_save\": true, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"round_robin_gradients\": false, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2022-01-15 19:21:34,494] [INFO] [config.py:1062:print]   zero_enabled ................. True\n",
            "[2022-01-15 19:21:34,495] [INFO] [config.py:1062:print]   zero_optimization_stage ...... 3\n",
            "[2022-01-15 19:21:34,496] [INFO] [config.py:1070:print]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 7e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.1\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 7e-05, \n",
            "            \"warmup_num_steps\": 26\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": 4.194304e+06, \n",
            "        \"stage3_prefetch_bucket_size\": 3.774874e+06, \n",
            "        \"stage3_param_persistence_threshold\": 2.048000e+04, \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 32, \n",
            "    \"gradient_clipping\": 0.5, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_batch_size\": 1.024000e+03, \n",
            "    \"train_micro_batch_size_per_gpu\": 32, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 53555\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1024\n",
            "  Gradient Accumulation steps = 32\n",
            "  Total optimization steps = 520\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py37_cu111 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.002501249313354492 seconds\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='88' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 88/520 1:38:35 < 8:15:16, 0.01 it/s, Epoch 1.67/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.142578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 17913\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./checkpoints/checkpoint-52\n",
            "Configuration saved in ./checkpoints/checkpoint-52/config.json\n",
            "Model weights saved in ./checkpoints/checkpoint-52/pytorch_model.bin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2022-01-15 20:16:21,740] [INFO] [engine.py:3053:save_fp16_model] Saving model weights to ./checkpoints/checkpoint-52/pytorch_model.bin\n",
            "[2022-01-15 20:16:34,716] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 20:19:33,347] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2022-01-15 20:19:33,392] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 20:22:29,384] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-52/global_step52/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2022-01-15 20:37:01,733] [INFO] [timer.py:184:stop] 0/2000, SamplesPerSec=17.746132318863697\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/520 4:23:01 < 5:45:13, 0.01 it/s, Epoch 4.32/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.142578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.140259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.139404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.144653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 17913\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./checkpoints/checkpoint-104\n",
            "Configuration saved in ./checkpoints/checkpoint-104/config.json\n",
            "Model weights saved in ./checkpoints/checkpoint-104/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 21:22:23,566] [INFO] [engine.py:3053:save_fp16_model] Saving model weights to ./checkpoints/checkpoint-104/pytorch_model.bin\n",
            "[2022-01-15 21:22:39,191] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 21:25:32,258] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2022-01-15 21:25:32,284] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 21:28:28,019] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-104/global_step104/zero_pp_rank_0_mp_rank_00_optim_states.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deleting older checkpoint [checkpoints/checkpoint-52] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 21:48:04,845] [INFO] [timer.py:184:stop] 0/4000, SamplesPerSec=17.708308059064347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 17913\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./checkpoints/checkpoint-156\n",
            "Configuration saved in ./checkpoints/checkpoint-156/config.json\n",
            "Model weights saved in ./checkpoints/checkpoint-156/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 22:23:10,688] [INFO] [engine.py:3053:save_fp16_model] Saving model weights to ./checkpoints/checkpoint-156/pytorch_model.bin\n",
            "[2022-01-15 22:23:25,860] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 22:26:20,481] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2022-01-15 22:26:21,453] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 22:29:15,343] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-156/global_step156/zero_pp_rank_0_mp_rank_00_optim_states.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deleting older checkpoint [checkpoints/checkpoint-104] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 22:58:20,005] [INFO] [timer.py:184:stop] 0/6000, SamplesPerSec=17.76980356549299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 17913\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to ./checkpoints/checkpoint-208\n",
            "Configuration saved in ./checkpoints/checkpoint-208/config.json\n",
            "Model weights saved in ./checkpoints/checkpoint-208/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 23:23:23,585] [INFO] [engine.py:3053:save_fp16_model] Saving model weights to ./checkpoints/checkpoint-208/pytorch_model.bin\n",
            "[2022-01-15 23:23:37,432] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-208/global_step209/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 23:26:31,650] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-208/global_step209/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2022-01-15 23:26:31,676] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: ./checkpoints/checkpoint-208/global_step209/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 23:29:27,237] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved ./checkpoints/checkpoint-208/global_step209/zero_pp_rank_0_mp_rank_00_optim_states.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deleting older checkpoint [checkpoints/checkpoint-156] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtput_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_optimization_partition_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                     \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m                 )\n\u001b[1;32m    622\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected keyword arguments: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                         \u001b[0;31m# None for past_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage3.py\u001b[0m in \u001b[0;36m_pre_forward_module_hook\u001b[0;34m(module, *args)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_pre_forward_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_sub_module_forward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_post_forward_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage3.py\u001b[0m in \u001b[0;36mpre_sub_module_forward_function\u001b[0;34m(self, sub_module)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_sub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m         see_memory_usage(\n\u001b[1;32m   1589\u001b[0m             \u001b[0;34mf\"Before sub module function {sub_module.__class__.__name__} after fetch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage3.py\u001b[0m in \u001b[0;36mfetch_sub_module\u001b[0;34m(self, sub_module)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# parameters are partitioned and need to be allgathered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitioned_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# parameters are inflight and communication needs to be completed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage3.py\u001b[0m in \u001b[0;36m_all_gather\u001b[0;34m(self, partitioned_params, async_op)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mparam_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioned_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0masync_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 hierarchy=self.hierarchy) if partitioned_params else None\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/partition_parameters.py\u001b[0m in \u001b[0;36mall_gather\u001b[0;34m(param_list, async_op, hierarchy)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mparam_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhierarchy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_been_updated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/partition_parameters.py\u001b[0m in \u001b[0;36m_all_gather\u001b[0;34m(self, param_list, async_op, hierarchy)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;31m# ret_value = self._allgather_params(all_gather_list, hierarchy=hierarchy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mret_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allgather_params_coalesced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_gather_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_gather_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/partition_parameters.py\u001b[0m in \u001b[0;36m_allgather_params_coalesced\u001b[0;34m(self, param_list, hierarchy)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mpartition_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_numel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mlocal_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;31m# allocate memory for allgather params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXqTv8kqxpJ"
      },
      "source": [
        "## save & convert\n",
        "\n",
        "After training is done you can save the model by calling `save_model()`. This will save the trained model to our `output_dir` from our `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl8s5qAseS6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "cellView": "form",
        "outputId": "919e9e5d-7eff-4c81-cb0d-0b81298c34c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final model file will be saved to /content/final_model\n"
          ]
        }
      ],
      "source": [
        "#@markdown prep work - imports, create directories for conversion\n",
        "import os\n",
        "from os.path import join\n",
        "from google.colab import files \n",
        "import gc\n",
        "fin_zero = join(os.getcwd(), \"final_zero_weights\")\n",
        "os.makedirs(fin_zero, exist_ok=True )\n",
        "\n",
        "fin_loc = join(os.getcwd(), \"final_model\")\n",
        "os.makedirs(fin_loc, exist_ok=True )\n",
        "\n",
        "print(f\"final model file will be saved to {fin_loc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "_in = Path('/content/checkpoints/config.json')\n",
        "_out = Path(fin_loc) / _in.name\n",
        "shutil.copyfile(_in, _out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0MN_2QkF3mMR",
        "outputId": "201449b6-19f3-4ddc-d998-ceb599a6a2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/final_model/config.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5quyGeMNdjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "4d42f662-b072-49f1-c57e-b77c986daebb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/final_zero_weights\n",
            "Configuration saved in /content/final_zero_weights/config.json\n",
            "Model weights saved in /content/final_zero_weights/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-01-15 23:46:22,882] [INFO] [engine.py:3053:save_fp16_model] Saving model weights to /content/final_zero_weights/pytorch_model.bin\n",
            "[2022-01-15 23:46:36,242] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: /content/final_zero_weights/global_step226/zero_pp_rank_0_mp_rank_00_model_states.pt\n",
            "[2022-01-15 23:49:30,873] [INFO] [engine.py:2947:_save_zero_checkpoint] zero checkpoint saved /content/final_zero_weights/global_step226/zero_pp_rank_0_mp_rank_00_optim_states.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 17913\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/520 4:23:01 < 5:45:13, 0.01 it/s, Epoch 4.32/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.142578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.140259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.139404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.144653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='107' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [107/560 00:50 < 03:34, 2.11 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.save_model(output_dir=fin_zero) # save to one directory with zero weights\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R9OFizppKuA"
      },
      "outputs": [],
      "source": [
        "del trainer \n",
        "gc.collect() # free up RAM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown convert zero weights\n",
        "!python /content/final_zero_weights/zero_to_fp32.py /content/checkpoints/final_zero_weights /content/final_model/pytorch_model.bin"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TV1ApL9MyacQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"yellow\"> note to self - may need to copy over other files like the config.json and so forth </font>"
      ],
      "metadata": {
        "id": "Q59C7PhR3tUA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoL32bnudOOz"
      },
      "source": [
        "# Save and Share\n",
        "\n",
        "- [transformers docs](https://huggingface.co/docs/transformers/model_sharing) on how to share it.\n",
        "\n",
        "if issues, try just working with the cells as a pseudo command-line a la:\n",
        "\n",
        "```\n",
        "!huggingface-cli login\n",
        "!git config --global credential.helper store\n",
        "!transformers-cli repo create your-model-name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22KKsVb8qIfX"
      },
      "source": [
        "### push tokenizer to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRa9mAYVrYZK"
      },
      "outputs": [],
      "source": [
        "#@markdown re-load tokenizer from the original model if something happened and it \n",
        "#@markdown does not exist\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "if \"_tokenizer\" not in globals():\n",
        "    _tokenizer = AutoTokenizer.from_pretrained(hf_name, use_fast=False,\n",
        "                                            max_length=2048,\n",
        "                                            model_max_length=2048,\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYZ5q5nlkESB"
      },
      "outputs": [],
      "source": [
        "# Push the tokenizer to your namespace with the name full_out_name with no local clone.\n",
        "_tokenizer.push_to_hub(full_out_name,\n",
        "                       use_auth_token=True,\n",
        "                       use_temp_dir=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o837RJbkqEzG"
      },
      "source": [
        "### push model to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZNmKryij7Jc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(fin_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwKYRf-ArMZh"
      },
      "outputs": [],
      "source": [
        "finetuned_model.push_to_hub(full_out_name,\n",
        "                            use_auth_token=True,\n",
        "                            use_temp_dir=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq9Uf9tOMM4_"
      },
      "source": [
        "### save to google drive \n",
        "\n",
        "- this is not required, just a utility in case pushing to hf is not working or for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gc80eZ39MP2W"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "save_gdrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "def get_timestamp(exact=False):\n",
        "    \"\"\"\n",
        "    get_timestamp - return a timestamp in the format YYYY-MM-DD_HH-MM-SS (exact=False)\n",
        "        or YYYY-MM-DD_HH-MM-SS-MS (exact=True)\n",
        "    exact : bool, optional, by default False,  if True, return a timestamp with seconds\n",
        "    \"\"\"\n",
        "    ts = (\n",
        "        datetime.now().strftime(\"%b-%d-%Y_-%H-%M-%S\")\n",
        "        if exact\n",
        "        else datetime.now().strftime(\"%b-%d-%Y_-%H\")\n",
        "    )\n",
        "    return ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxptzHpOJUo_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown connect to your drive with `google.colab`\n",
        "from google.colab import drive\n",
        "if save_gdrive:\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BvXXekBJpjo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown save and copy the directory to drive\n",
        "import os, shutil\n",
        "from os.path import join\n",
        "if save_gdrive:\n",
        "    drive_folder = join('content', 'drive', \"Programming\", get_timestamp(), full_out_name)\n",
        "    os.makedirs(drive_folder, exist_ok=True )\n",
        "    outpath = shutil.copytree(fin_loc, join(drive_folder, \"new\"))\n",
        "    print(outpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkQTGd3nsrk"
      },
      "source": [
        "# testing the new model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown print GPU status\n",
        "import torch\n",
        "!nvidia-smi\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"\\nwill run computations on {device}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tUzAUbOtvvfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XV2H0bfXsZu"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "my_chatbot = pipeline('text-generation', \n",
        "                      model=finetuned_model, tokenizer=_tokenizer,\n",
        "                      device=0 if device == 'cuda' else -1,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_chatbot(\"hello\")"
      ],
      "metadata": {
        "id": "XP7wY1cXv-OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeV4O82xlbEZ"
      },
      "source": [
        "## add prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nqQWB_r0ndUd"
      },
      "outputs": [],
      "source": [
        "#@title define speaker and responder\n",
        "#@markdown for testing the models this should not need to be changed. \n",
        "#@markdown if testing a model related to [ai-msgbot](https://github.com/pszemraj/ai-msgbot)\n",
        "#@markdown trained on data that **was not** using the entries below, update as needed.\n",
        "speaker = \"person alpha\" #@param {type:\"string\"}\n",
        "responder = \"person beta\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPaBftn6qEo7"
      },
      "source": [
        "## define prompt messages\n",
        "\n",
        "the reason `f\"{responder}:\\n\"` is added at the end of each prompt is to force the text-gen model to actually _respond_ to the prompt as opposed to adding on to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdGJdXw3dUr"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompts = [\n",
        "           [f\"{speaker}:\\n\", \"hi! how are you doing?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"what should I bring to the party?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"do you like memes?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"can we go on a date together this weekend?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"what's up homie?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"do you know how can I make friends here?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"so what do you like to do for fun?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"what is your favorite brand of cereal?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "           [f\"{speaker}:\\n\", \"what is the meaning of existence?\\n\", \"\\n\", f\"{responder}:\\n\"],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hOmGElvoJHe"
      },
      "source": [
        "## generate text!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wsAeVa6HzuZ2"
      },
      "outputs": [],
      "source": [
        "#@markdown set amount of text to generate (higher # = longer RT)\n",
        "resp_len =  1024#@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPu3IcQ1oOQT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title model generated chats\n",
        "#@markdown - note that responses output the prompt as part of the output (and that counts \n",
        "#@markdown for part of the max length reqs)\n",
        "for i, prompt in enumerate(prompts):\n",
        "    this_prompt = \"\".join(prompt)\n",
        "    result = my_chatbot(\n",
        "                        this_prompt, \n",
        "                        do_sample=True,\n",
        "                        top_k=50,\n",
        "                        top_p=0.9, \n",
        "                        min_length=len(this_prompt) + resp_len,\n",
        "                        no_repeat_ngram_size=3,\n",
        "                    )\n",
        "    \n",
        "    print(f\"==========Testing Prompt-ID #{i} ==========\")\n",
        "    print(f\"PROMPT TEXT:\\n{''.join(prompt)}\")\n",
        "    print(\"----------FULL GENERATED TEXT:\")\n",
        "    print(result[0]['generated_text'])\n",
        "    print(\"\\n\" * 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ZGank_Dr4k"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "- export all the parameters you could ever want, and more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9PP84fDu3M"
      },
      "outputs": [],
      "source": [
        "metadata = training_args.to_sanitized_dict()\n",
        "metadata[\"configs_src\"] = hf_name\n",
        "pp.pprint(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlh-pkTNTIyX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "metadata_path = f\"{model_name_header}_training_metadata.json\"\n",
        "with open(metadata_path, \"w\") as write_file:\n",
        "    json.dump(metadata, write_file)\n",
        "\n",
        "files.download(metadata_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ovY6T2YwV5t"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUxn7x7UwWV8"
      },
      "source": [
        "## full docstring for TrainingArgs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd5_kWguwa2S"
      },
      "outputs": [],
      "source": [
        "ta_docstring = \"\"\"\n",
        "    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
        "    itself**.\n",
        "    Using :class:`~transformers.HfArgumentParser` we can turn this class into `argparse\n",
        "    <https://docs.python.org/3/library/argparse.html#module-argparse>`__ arguments that can be specified on the command\n",
        "    line.\n",
        "    Parameters:\n",
        "        output_dir (:obj:`str`):\n",
        "            The output directory where the model predictions and checkpoints will be written.\n",
        "        overwrite_output_dir (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            If :obj:`True`, overwrite the content of the output directory. Use this to continue training if\n",
        "            :obj:`output_dir` points to a checkpoint directory.\n",
        "        do_train (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to run training or not. This argument is not directly used by :class:`~transformers.Trainer`, it's\n",
        "            intended to be used by your training/evaluation scripts instead. See the `example scripts\n",
        "            <https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.\n",
        "        do_eval (:obj:`bool`, `optional`):\n",
        "            Whether to run evaluation on the validation set or not. Will be set to :obj:`True` if\n",
        "            :obj:`evaluation_strategy` is different from :obj:`\"no\"`. This argument is not directly used by\n",
        "            :class:`~transformers.Trainer`, it's intended to be used by your training/evaluation scripts instead. See\n",
        "            the `example scripts <https://github.com/huggingface/transformers/tree/master/examples>`__ for more\n",
        "            details.\n",
        "        do_predict (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to run predictions on the test set or not. This argument is not directly used by\n",
        "            :class:`~transformers.Trainer`, it's intended to be used by your training/evaluation scripts instead. See\n",
        "            the `example scripts <https://github.com/huggingface/transformers/tree/master/examples>`__ for more\n",
        "            details.\n",
        "        evaluation_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`\"no\"`):\n",
        "            The evaluation strategy to adopt during training. Possible values are:\n",
        "                * :obj:`\"no\"`: No evaluation is done during training.\n",
        "                * :obj:`\"steps\"`: Evaluation is done (and logged) every :obj:`eval_steps`.\n",
        "                * :obj:`\"epoch\"`: Evaluation is done at the end of each epoch.\n",
        "        prediction_loss_only (:obj:`bool`, `optional`, defaults to `False`):\n",
        "            When performing evaluation and generating predictions, only returns the loss.\n",
        "        per_device_train_batch_size (:obj:`int`, `optional`, defaults to 8):\n",
        "            The batch size per GPU/TPU core/CPU for training.\n",
        "        per_device_eval_batch_size (:obj:`int`, `optional`, defaults to 8):\n",
        "            The batch size per GPU/TPU core/CPU for evaluation.\n",
        "        gradient_accumulation_steps (:obj:`int`, `optional`, defaults to 1):\n",
        "            Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "            .. warning::\n",
        "                When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n",
        "                logging, evaluation, save will be conducted every ``gradient_accumulation_steps * xxx_step`` training\n",
        "                examples.\n",
        "        eval_accumulation_steps (:obj:`int`, `optional`):\n",
        "            Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
        "            left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but\n",
        "            requires more memory).\n",
        "        learning_rate (:obj:`float`, `optional`, defaults to 5e-5):\n",
        "            The initial learning rate for :class:`~transformers.AdamW` optimizer.\n",
        "        weight_decay (:obj:`float`, `optional`, defaults to 0):\n",
        "            The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in\n",
        "            :class:`~transformers.AdamW` optimizer.\n",
        "        adam_beta1 (:obj:`float`, `optional`, defaults to 0.9):\n",
        "            The beta1 hyperparameter for the :class:`~transformers.AdamW` optimizer.\n",
        "        adam_beta2 (:obj:`float`, `optional`, defaults to 0.999):\n",
        "            The beta2 hyperparameter for the :class:`~transformers.AdamW` optimizer.\n",
        "        adam_epsilon (:obj:`float`, `optional`, defaults to 1e-8):\n",
        "            The epsilon hyperparameter for the :class:`~transformers.AdamW` optimizer.\n",
        "        max_grad_norm (:obj:`float`, `optional`, defaults to 1.0):\n",
        "            Maximum gradient norm (for gradient clipping).\n",
        "        num_train_epochs(:obj:`float`, `optional`, defaults to 3.0):\n",
        "            Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
        "            the last epoch before stopping training).\n",
        "        max_steps (:obj:`int`, `optional`, defaults to -1):\n",
        "            If set to a positive number, the total number of training steps to perform. Overrides\n",
        "            :obj:`num_train_epochs`. In case of using a finite iterable dataset the training may stop before reaching\n",
        "            the set number of steps when all data is exhausted\n",
        "        lr_scheduler_type (:obj:`str` or :class:`~transformers.SchedulerType`, `optional`, defaults to :obj:`\"linear\"`):\n",
        "            The scheduler type to use. See the documentation of :class:`~transformers.SchedulerType` for all possible\n",
        "            values.\n",
        "        warmup_ratio (:obj:`float`, `optional`, defaults to 0.0):\n",
        "            Ratio of total training steps used for a linear warmup from 0 to :obj:`learning_rate`.\n",
        "        warmup_steps (:obj:`int`, `optional`, defaults to 0):\n",
        "            Number of steps used for a linear warmup from 0 to :obj:`learning_rate`. Overrides any effect of\n",
        "            :obj:`warmup_ratio`.\n",
        "        log_level (:obj:`str`, `optional`, defaults to ``passive``):\n",
        "            Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
        "            'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the\n",
        "            application set the level.\n",
        "        log_level_replica (:obj:`str`, `optional`, defaults to ``passive``):\n",
        "            Logger log level to use on replicas. Same choices as ``log_level``\"\n",
        "        log_on_each_node (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            In multinode distributed training, whether to log using :obj:`log_level` once per node, or only on the main\n",
        "            node.\n",
        "        logging_dir (:obj:`str`, `optional`):\n",
        "            `TensorBoard <https://www.tensorflow.org/tensorboard>`__ log directory. Will default to\n",
        "            `output_dir/runs/**CURRENT_DATETIME_HOSTNAME**`.\n",
        "        logging_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`\"steps\"`):\n",
        "            The logging strategy to adopt during training. Possible values are:\n",
        "                * :obj:`\"no\"`: No logging is done during training.\n",
        "                * :obj:`\"epoch\"`: Logging is done at the end of each epoch.\n",
        "                * :obj:`\"steps\"`: Logging is done every :obj:`logging_steps`.\n",
        "        logging_first_step (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to log and evaluate the first :obj:`global_step` or not.\n",
        "        logging_steps (:obj:`int`, `optional`, defaults to 500):\n",
        "            Number of update steps between two logs if :obj:`logging_strategy=\"steps\"`.\n",
        "        logging_nan_inf_filter (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to filter :obj:`nan` and :obj:`inf` losses for logging. If set to obj:`True` the loss of every step\n",
        "            that is :obj:`nan` or :obj:`inf` is filtered and the average loss of the current logging window is taken\n",
        "            instead.\n",
        "            .. note::\n",
        "                :obj:`logging_nan_inf_filter` only influences the logging of loss values, it does not change the\n",
        "                behavior the gradient is computed or applied to the model.\n",
        "        save_strategy (:obj:`str` or :class:`~transformers.trainer_utils.IntervalStrategy`, `optional`, defaults to :obj:`\"steps\"`):\n",
        "            The checkpoint save strategy to adopt during training. Possible values are:\n",
        "                * :obj:`\"no\"`: No save is done during training.\n",
        "                * :obj:`\"epoch\"`: Save is done at the end of each epoch.\n",
        "                * :obj:`\"steps\"`: Save is done every :obj:`save_steps`.\n",
        "        save_steps (:obj:`int`, `optional`, defaults to 500):\n",
        "            Number of updates steps before two checkpoint saves if :obj:`save_strategy=\"steps\"`.\n",
        "        save_total_limit (:obj:`int`, `optional`):\n",
        "            If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
        "            :obj:`output_dir`.\n",
        "        save_on_each_node (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
        "            the main one.\n",
        "            This should not be activated when the different nodes use the same storage as the files will be saved with\n",
        "            the same names for each node.\n",
        "        no_cuda (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to not use CUDA even when it is available or not.\n",
        "        seed (:obj:`int`, `optional`, defaults to 42):\n",
        "            Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
        "            :func:`~transformers.Trainer.model_init` function to instantiate the model if it has some randomly\n",
        "            initialized parameters.\n",
        "        bf16 (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
        "            NVIDIA architecture. This is an experimental API and it may change.\n",
        "        fp16 (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
        "        fp16_opt_level (:obj:`str`, `optional`, defaults to 'O1'):\n",
        "            For :obj:`fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details\n",
        "            on the `Apex documentation <https://nvidia.github.io/apex/amp.html>`__.\n",
        "        fp16_backend (:obj:`str`, `optional`, defaults to :obj:`\"auto\"`):\n",
        "            This argument is deprecated. Use ``half_precision_backend`` instead.\n",
        "        half_precision_backend (:obj:`str`, `optional`, defaults to :obj:`\"auto\"`):\n",
        "            The backend to use for mixed precision training. Must be one of :obj:`\"auto\"`, :obj:`\"amp\"` or\n",
        "            :obj:`\"apex\"`. :obj:`\"auto\"` will use AMP or APEX depending on the PyTorch version detected, while the\n",
        "            other choices will force the requested backend.\n",
        "        bf16_full_eval (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
        "            metric values. This is an experimental API and it may change.\n",
        "        fp16_full_eval (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
        "            metric values.\n",
        "        tf32 (:obj:`bool`, `optional`):\n",
        "            Whether to enable tf32 mode, available in Ampere and newer GPU architectures. This is an experimental API\n",
        "            and it may change.\n",
        "        local_rank (:obj:`int`, `optional`, defaults to -1):\n",
        "            Rank of the process during distributed training.\n",
        "        xpu_backend (:obj:`str`, `optional`):\n",
        "            The backend to use for xpu distributed training. Must be one of :obj:`\"mpi\"` or :obj:`\"ccl\"`.\n",
        "        tpu_num_cores (:obj:`int`, `optional`):\n",
        "            When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
        "        dataloader_drop_last (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
        "            or not.\n",
        "        eval_steps (:obj:`int`, `optional`):\n",
        "            Number of update steps between two evaluations if :obj:`evaluation_strategy=\"steps\"`. Will default to the\n",
        "            same value as :obj:`logging_steps` if not set.\n",
        "        dataloader_num_workers (:obj:`int`, `optional`, defaults to 0):\n",
        "            Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
        "            main process.\n",
        "        past_index (:obj:`int`, `optional`, defaults to -1):\n",
        "            Some models like :doc:`TransformerXL <../model_doc/transformerxl>` or :doc:`XLNet <../model_doc/xlnet>` can\n",
        "            make use of the past hidden states for their predictions. If this argument is set to a positive int, the\n",
        "            ``Trainer`` will use the corresponding output (usually index 2) as the past state and feed it to the model\n",
        "            at the next training step under the keyword argument ``mems``.\n",
        "        run_name (:obj:`str`, `optional`):\n",
        "            A descriptor for the run. Typically used for `wandb <https://www.wandb.com/>`_ logging.\n",
        "        disable_tqdm (:obj:`bool`, `optional`):\n",
        "            Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
        "            :class:`~transformers.notebook.NotebookTrainingTracker` in Jupyter Notebooks. Will default to :obj:`True`\n",
        "            if the logging level is set to warn or lower (default), :obj:`False` otherwise.\n",
        "        remove_unused_columns (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            If using :obj:`datasets.Dataset` datasets, whether or not to automatically remove the columns unused by the\n",
        "            model forward method.\n",
        "            (Note that this behavior is not implemented for :class:`~transformers.TFTrainer` yet.)\n",
        "        label_names (:obj:`List[str]`, `optional`):\n",
        "            The list of keys in your dictionary of inputs that correspond to the labels.\n",
        "            Will eventually default to :obj:`[\"labels\"]` except if the model used is one of the\n",
        "            :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`[\"start_positions\",\n",
        "            \"end_positions\"]`.\n",
        "        load_best_model_at_end (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether or not to load the best model found during training at the end of training.\n",
        "            .. note::\n",
        "                When set to :obj:`True`, the parameters :obj:`save_strategy` needs to be the same as\n",
        "                :obj:`eval_strategy`, and in the case it is \"steps\", :obj:`save_steps` must be a round multiple of\n",
        "                :obj:`eval_steps`.\n",
        "        metric_for_best_model (:obj:`str`, `optional`):\n",
        "            Use in conjunction with :obj:`load_best_model_at_end` to specify the metric to use to compare two different\n",
        "            models. Must be the name of a metric returned by the evaluation with or without the prefix :obj:`\"eval_\"`.\n",
        "            Will default to :obj:`\"loss\"` if unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation\n",
        "            loss).\n",
        "            If you set this value, :obj:`greater_is_better` will default to :obj:`True`. Don't forget to set it to\n",
        "            :obj:`False` if your metric is better when lower.\n",
        "        greater_is_better (:obj:`bool`, `optional`):\n",
        "            Use in conjunction with :obj:`load_best_model_at_end` and :obj:`metric_for_best_model` to specify if better\n",
        "            models should have a greater metric or not. Will default to:\n",
        "            - :obj:`True` if :obj:`metric_for_best_model` is set to a value that isn't :obj:`\"loss\"` or\n",
        "              :obj:`\"eval_loss\"`.\n",
        "            - :obj:`False` if :obj:`metric_for_best_model` is not set, or set to :obj:`\"loss\"` or :obj:`\"eval_loss\"`.\n",
        "        ignore_data_skip (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
        "            stage as in the previous training. If set to :obj:`True`, the training will begin faster (as that skipping\n",
        "            step can take a long time) but will not yield the same results as the interrupted training would have.\n",
        "        sharded_ddp (:obj:`bool`, :obj:`str` or list of :class:`~transformers.trainer_utils.ShardedDDPOption`, `optional`, defaults to :obj:`False`):\n",
        "            Use Sharded DDP training from `FairScale <https://github.com/facebookresearch/fairscale>`__ (in distributed\n",
        "            training only). This is an experimental feature.\n",
        "            A list of options along the following:\n",
        "            - :obj:`\"simple\"`: to use first instance of sharded DDP released by fairscale (:obj:`ShardedDDP`) similar\n",
        "              to ZeRO-2.\n",
        "            - :obj:`\"zero_dp_2\"`: to use the second instance of sharded DPP released by fairscale\n",
        "              (:obj:`FullyShardedDDP`) in Zero-2 mode (with :obj:`reshard_after_forward=False`).\n",
        "            - :obj:`\"zero_dp_3\"`: to use the second instance of sharded DPP released by fairscale\n",
        "              (:obj:`FullyShardedDDP`) in Zero-3 mode (with :obj:`reshard_after_forward=True`).\n",
        "            - :obj:`\"offload\"`: to add ZeRO-offload (only compatible with :obj:`\"zero_dp_2\"` and :obj:`\"zero_dp_3\"`).\n",
        "            If a string is passed, it will be split on space. If a bool is passed, it will be converted to an empty\n",
        "            list for :obj:`False` and :obj:`[\"simple\"]` for :obj:`True`.\n",
        "        deepspeed (:obj:`str` or :obj:`dict`, `optional`):\n",
        "            Use `Deepspeed <https://github.com/microsoft/deepspeed>`__. This is an experimental feature and its API may\n",
        "            evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
        "            ``ds_config.json``) or an already loaded json file as a :obj:`dict`\"\n",
        "        label_smoothing_factor (:obj:`float`, `optional`, defaults to 0.0):\n",
        "            The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
        "            labels are changed from 0s and 1s to :obj:`label_smoothing_factor/num_labels` and :obj:`1 -\n",
        "            label_smoothing_factor + label_smoothing_factor/num_labels` respectively.\n",
        "        debug (:obj:`str` or list of :class:`~transformers.debug_utils.DebugOption`, `optional`, defaults to :obj:`\"\"`):\n",
        "            Enable one or more debug features. This is an experimental feature.\n",
        "            Possible options are:\n",
        "            - :obj:`\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that\n",
        "              led to the event\n",
        "            - :obj:`\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
        "            The options should be separated by whitespaces.\n",
        "        adafactor (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether or not to use the :class:`~transformers.Adafactor` optimizer instead of\n",
        "            :class:`~transformers.AdamW`.\n",
        "        group_by_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
        "            padding applied and be more efficient). Only useful if applying dynamic padding.\n",
        "        length_column_name (:obj:`str`, `optional`, defaults to :obj:`\"length\"`):\n",
        "            Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
        "            than computing them on train startup. Ignored unless :obj:`group_by_length` is :obj:`True` and the dataset\n",
        "            is an instance of :obj:`Dataset`.\n",
        "        report_to (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`\"all\"`):\n",
        "            The list of integrations to report the results and logs to. Supported platforms are :obj:`\"azure_ml\"`,\n",
        "            :obj:`\"comet_ml\"`, :obj:`\"mlflow\"`, :obj:`\"tensorboard\"` and :obj:`\"wandb\"`. Use :obj:`\"all\"` to report to\n",
        "            all integrations installed, :obj:`\"none\"` for no integrations.\n",
        "        ddp_find_unused_parameters (:obj:`bool`, `optional`):\n",
        "            When using distributed training, the value of the flag :obj:`find_unused_parameters` passed to\n",
        "            :obj:`DistributedDataParallel`. Will default to :obj:`False` if gradient checkpointing is used, :obj:`True`\n",
        "            otherwise.\n",
        "        dataloader_pin_memory (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether you want to pin memory in data loaders or not. Will default to :obj:`True`.\n",
        "        skip_memory_metrics (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "            Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
        "            down the training and evaluation speed.\n",
        "        push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            Whether or not to upload the trained model to the hub after training. If this is activated, and\n",
        "            :obj:`output_dir` exists, it needs to be a local clone of the repository to which the\n",
        "            :class:`~transformers.Trainer` will be pushed.\n",
        "        resume_from_checkpoint (:obj:`str`, `optional`):\n",
        "            The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
        "            :class:`~transformers.Trainer`, it's intended to be used by your training/evaluation scripts instead. See\n",
        "            the `example scripts <https://github.com/huggingface/transformers/tree/master/examples>`__ for more\n",
        "            details.\n",
        "        hub_model_id (:obj:`str`, `optional`):\n",
        "            The name of the repository to keep in sync with the local `output_dir`. It can be a simple model ID in\n",
        "            which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
        "            for instance :obj:`\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
        "            :obj:`\"organization_name/model\"`. Will default to :obj:`user_name/output_dir_name` with `output_dir_name`\n",
        "            being the name of :obj:`output_dir`.\n",
        "            Will default to to the name of :obj:`output_dir`.\n",
        "        hub_strategy (:obj:`str` or :class:`~transformers.trainer_utils.HubStrategy`, `optional`, defaults to :obj:`\"every_save\"`):\n",
        "            Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
        "            - :obj:`\"end\"`: push the model, its configuration, the tokenizer (if passed along to the\n",
        "              :class:`~transformers.Trainer`) and a draft of a model card at the end of training.\n",
        "            - :obj:`\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the\n",
        "              :class:`~transformers.Trainer`) and a draft of a model card each time there is a model save. The pushes\n",
        "              are asynchronous to not block training, and in case the save are very frequent, a new push is only\n",
        "              attempted if the previous one is finished. A last push is made with the final model at the end of\n",
        "              training.\n",
        "            - :obj:`\"checkpoint\"`: like :obj:`\"every_save\"` but the latest checkpoint is also pushed in a subfolder\n",
        "              named last-checkpoint, allowing you to resume training easily with\n",
        "              :obj:`trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
        "            - :obj:`\"all_checkpoints\"`: like :obj:`\"checkpoint\"` but all checkpoints are pushed like they appear in the\n",
        "              output folder (so you will get one checkpoint folder per folder in your final repository)\n",
        "        hub_token (:obj:`str`, `optional`):\n",
        "            The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
        "            :obj:`huggingface-cli login`.\n",
        "        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P6c-YoTwj79"
      },
      "outputs": [],
      "source": [
        "import pprint as pp\n",
        "\n",
        "pp.pprint(ta_docstring)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvITcdBBwn8C"
      },
      "source": [
        "## SchedulerType for learning rate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_lUZiWJwm-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SchedulerType(ExplicitEnum):\n",
        "    LINEAR = \"linear\"\n",
        "    COSINE = \"cosine\"\n",
        "    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
        "    POLYNOMIAL = \"polynomial\"\n",
        "    CONSTANT = \"constant\"\n",
        "    CONSTANT_WITH_WARMUP = \"constant_with_warmup\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7ovY6T2YwV5t"
      ],
      "machine_shape": "hm",
      "name": "Train GPT for Conversation w Huggingface and Deepspeed",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a33ac2f15e44d7eaefd115832ee82fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9060157f56db472e8b758da2a3f7c833",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba2f1e713ccf4d5da90943fb50a09927",
              "IPY_MODEL_8167a4e596dd4920af60238db8d2412d",
              "IPY_MODEL_f61eea0da924472897d21e4735ab3105"
            ]
          }
        },
        "9060157f56db472e8b758da2a3f7c833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba2f1e713ccf4d5da90943fb50a09927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d1641f7633c84088a7f670312deda1d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc7136de1ada4c2a9dffe22536a1b631"
          }
        },
        "8167a4e596dd4920af60238db8d2412d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_536004f38fbf407a94b8d6b1575aa194",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 682,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 682,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74676f70cf5a4458915f94e84ae07cc0"
          }
        },
        "f61eea0da924472897d21e4735ab3105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd4fb007e7624cea9f745e981a1e7b81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 682/682 [00:00&lt;00:00, 25.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43293ece0f6b482888e2aadff842bec6"
          }
        },
        "d1641f7633c84088a7f670312deda1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc7136de1ada4c2a9dffe22536a1b631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "536004f38fbf407a94b8d6b1575aa194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74676f70cf5a4458915f94e84ae07cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd4fb007e7624cea9f745e981a1e7b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43293ece0f6b482888e2aadff842bec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1af3a61b01964a448b2c066b1f4c99eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_329488f885444705b1345415c82f583a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c29ff2da3cf44fb8da08c5be4226b04",
              "IPY_MODEL_aba558da08124e1c98cd548a6bee4a5e",
              "IPY_MODEL_8b8f7311056744ccb67952270fe32a37"
            ]
          }
        },
        "329488f885444705b1345415c82f583a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c29ff2da3cf44fb8da08c5be4226b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c14cf90d94ad47218bcd488ffab1480f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_953c7ba481e149dba88bb44dc066367e"
          }
        },
        "aba558da08124e1c98cd548a6bee4a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_275133dc9e8648b8a6ec903630d09857",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7712fd32c66a48f18a03c8b0e52efe1f"
          }
        },
        "8b8f7311056744ccb67952270fe32a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9f76fa85494245c0b1d415709d456c2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 2.03MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d21fe997e5ca426c80859d6f01c5869a"
          }
        },
        "c14cf90d94ad47218bcd488ffab1480f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "953c7ba481e149dba88bb44dc066367e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "275133dc9e8648b8a6ec903630d09857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7712fd32c66a48f18a03c8b0e52efe1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f76fa85494245c0b1d415709d456c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d21fe997e5ca426c80859d6f01c5869a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "948c35c31a2247bb990750728424ac0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_766e587045434959bafaf580750bd62f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c777053ed1b449bc87c5b7fc2ed9ff76",
              "IPY_MODEL_dfc942da04e6408c95d61ae2211ae94a",
              "IPY_MODEL_d07eba7c5c29421ba8266c0ed91f4eff"
            ]
          }
        },
        "766e587045434959bafaf580750bd62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c777053ed1b449bc87c5b7fc2ed9ff76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3d57cf64827646b2ad0140e99370f0a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b930058869849d0921c6180e13ebb6f"
          }
        },
        "dfc942da04e6408c95d61ae2211ae94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d0f56714852040df9648d3a9147995d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a2b85655e984e97aa6d46484c5c0e03"
          }
        },
        "d07eba7c5c29421ba8266c0ed91f4eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0fab74f6f9348e89cd077674e3fbede",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 522kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a8141a298cb42079597e5f3be88b952"
          }
        },
        "3d57cf64827646b2ad0140e99370f0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b930058869849d0921c6180e13ebb6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0f56714852040df9648d3a9147995d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a2b85655e984e97aa6d46484c5c0e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0fab74f6f9348e89cd077674e3fbede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a8141a298cb42079597e5f3be88b952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a52cf3de0cec440c97775e75e9436eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a31dfc13d8ec44e5888e48ff7072a050",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41148e8d8cfb420b96dd548fc4a41c16",
              "IPY_MODEL_f062e208f0f6498d907f6c2be43f53da",
              "IPY_MODEL_b99402cdac3744ea86414f4ea29bf92b"
            ]
          }
        },
        "a31dfc13d8ec44e5888e48ff7072a050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41148e8d8cfb420b96dd548fc4a41c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b17a627055744668a515a357572d86e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b33c02e7b6c14a2a8b83c1be70ada332"
          }
        },
        "f062e208f0f6498d907f6c2be43f53da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c53f4c2fde134f4bbbe4ab8139918ef4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 90,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 90,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f030d8a1d87434997358fb2fd0a5443"
          }
        },
        "b99402cdac3744ea86414f4ea29bf92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4cf1972b56041c8a5bae16335bc5d26",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 90.0/90.0 [00:00&lt;00:00, 3.66kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60c7bd3253974cfc8adcb594a0e9565b"
          }
        },
        "b17a627055744668a515a357572d86e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b33c02e7b6c14a2a8b83c1be70ada332": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c53f4c2fde134f4bbbe4ab8139918ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f030d8a1d87434997358fb2fd0a5443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4cf1972b56041c8a5bae16335bc5d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60c7bd3253974cfc8adcb594a0e9565b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "095826aee8ff41f6b64ce8838006ee99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d555ee7fc28b48d99b4e253a769d5139",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_26fa7857940b400b8cacea3b8b772a40",
              "IPY_MODEL_43a651e7b1ac4e0e885b4a7f1fdd6b61",
              "IPY_MODEL_1432523c1da54e2ead897a2bc06e3624"
            ]
          }
        },
        "d555ee7fc28b48d99b4e253a769d5139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26fa7857940b400b8cacea3b8b772a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e72128ec36d6436789ef01c5b82f46f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08626a636f5545f89ff0e0ec84148638"
          }
        },
        "43a651e7b1ac4e0e885b4a7f1fdd6b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75a1bc7b111c467e9bf741aad1ce6c88",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1415,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1415,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_236d79159e8648d79e9ccae6d2e79532"
          }
        },
        "1432523c1da54e2ead897a2bc06e3624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d3e252073d74e9e86837e6433f168b0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.38k/1.38k [00:00&lt;00:00, 60.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7c70bb3d6dc14c6ca5f0572dc15d33a0"
          }
        },
        "e72128ec36d6436789ef01c5b82f46f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08626a636f5545f89ff0e0ec84148638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75a1bc7b111c467e9bf741aad1ce6c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "236d79159e8648d79e9ccae6d2e79532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d3e252073d74e9e86837e6433f168b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7c70bb3d6dc14c6ca5f0572dc15d33a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68d87caed46c4e039cf518143eb746e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a8e4f95ec884c18b092f0752869e762",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a39dab9b8d5044cc9bf0d9ecb142f485",
              "IPY_MODEL_49a1e02324604622a5d5e92d3f9eb8be",
              "IPY_MODEL_7ccbc92bec44416d832c5010023db41b"
            ]
          }
        },
        "2a8e4f95ec884c18b092f0752869e762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a39dab9b8d5044cc9bf0d9ecb142f485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f9bf7c9114e44fabe0a5e3e6a456d2b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7725fbdbdf244edc88f31c4f197cc614"
          }
        },
        "49a1e02324604622a5d5e92d3f9eb8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c5b2d88a6ae418e98e3d6c5fd492580",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5363100545,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5363100545,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba39b412860f414ba7b81dc79a752d83"
          }
        },
        "7ccbc92bec44416d832c5010023db41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e44fbd9d29a94ecab8f7e8b4788b8725",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.99G/4.99G [02:38&lt;00:00, 27.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c06da8044b84f17affd904515f2547f"
          }
        },
        "0f9bf7c9114e44fabe0a5e3e6a456d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7725fbdbdf244edc88f31c4f197cc614": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c5b2d88a6ae418e98e3d6c5fd492580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba39b412860f414ba7b81dc79a752d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e44fbd9d29a94ecab8f7e8b4788b8725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c06da8044b84f17affd904515f2547f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}