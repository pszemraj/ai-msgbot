{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/update-notebooks/notebooks/colab-huggingface-API/finetune_gpt_j_6B_8bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLzX_EPqAnEW"
      },
      "source": [
        "# <center> Fine-tuning 6-Billion param GPT-J in colab with LoRA and 8-bit compression </center>\n",
        "\n",
        "> goal: fine-tune a GPT-J 8-bit model on a custom dataset\n",
        "\n",
        "This notebook essentially combines the workflow presented in the huggingface documentation [here](https://huggingface.co/docs/transformers/training) with the work done in fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEvkyRhsHXK_"
      },
      "source": [
        "## system setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "846niMokUVnR"
      },
      "outputs": [],
      "source": [
        "#@title print GPU status\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LrDWdEzv3LaX"
      },
      "outputs": [],
      "source": [
        "#@markdown add auto-Colab formatting with `IPython.display`\n",
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HTRjl_eh2dUY"
      },
      "outputs": [],
      "source": [
        "#@title print out the VM's CPU stats\n",
        "#@markdown - a high-RAM runtime is recommended as the model file itself is around\n",
        "#@markdown 10 gb. That gets loaded into ram, so 12 gb RAM will not cut it\n",
        "from psutil import virtual_memory\n",
        "import os\n",
        "ram_gb = round(virtual_memory().total / (1024**3), 1)\n",
        "print(f'Runtime has {ram_gb} gigs of memory and {os.cpu_count()} processors')\n",
        "\n",
        "if ram_gb < 20: print(\"WARNING - your CPU RAM allocated is less than 20.\",\n",
        "                      \" You may experience errors loading\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFLqCkktTVjb"
      },
      "source": [
        "# setup - params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__m_tu2pD3Z"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AfCZ6IFBmBa6"
      },
      "outputs": [],
      "source": [
        "#@title Key User Inputs- Parameters\n",
        "#@markdown >Note that `hf_name` is the model for config/tokenizer and `model_8bit`\n",
        "#@markdown is for the model itself.\n",
        "\n",
        "SAVE_GDRIVE = False #@param {type:\"boolean\"}\n",
        "hf_name = \"EleutherAI/gpt-j-6B\" #@param {type:\"string\"}\n",
        "model_8bit = \"ethzanalytics/GPT-J-6B-8bit-Convo-D3E\" #@param {type:\"string\"}\n",
        "N_EPOCHS =  1#@param {type:\"number\"}\n",
        "model_name_header = \"Converse\" #@param {type:\"string\"}\n",
        "dataset = \"WoW\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0MKW3zKOZ24N"
      },
      "outputs": [],
      "source": [
        "#@markdown Logging\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "# for trainer\n",
        "from os.path import join\n",
        "os.makedirs(join(os.getcwd(), \"logs\"), exist_ok=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Op0GXmC8CCyR"
      },
      "outputs": [],
      "source": [
        "#@title **install primary packages**\n",
        "\n",
        "#@markdown These were all from the original notebook except for `datasets` \n",
        "!pip install -U transformers -q\n",
        "!pip install -U datasets -q\n",
        "!pip install bitsandbytes-cuda111==0.26.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-e4_-jeaMOmL"
      },
      "outputs": [],
      "source": [
        "#@markdown **install added packages**\n",
        "\n",
        "#@markdown primarily utils like joblib etc\n",
        "!pip install -U joblib\n",
        "!sudo apt-get install git-lfs\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RGLFHxeqp1E3"
      },
      "outputs": [],
      "source": [
        "#@markdown import packages (from deepspeed notebook)\n",
        "import os\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPTNeoForCausalLM\n",
        "from tqdm.auto import tqdm \n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p0dy1ZFwClcq"
      },
      "outputs": [],
      "source": [
        "#@markdown import packages (all the rest)\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPdCvWjPh3AP"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BDtwoDq3HvFh"
      },
      "outputs": [],
      "source": [
        "#@title <font color=\"orange\"> Sign in to HF </font>\n",
        "#@markdown create an account on their website if zou don't have one - you need somewhere to put this.\n",
        "\n",
        "#@markdown also imports a lot of the functions from the package\n",
        "from huggingface_hub import (\n",
        "    # User management\n",
        "    login,\n",
        "    logout,\n",
        "    notebook_login,\n",
        "    whoami,\n",
        "    # Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility,\n",
        "    # And some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eakOqE6ZJbFi"
      },
      "outputs": [],
      "source": [
        "#@markdown sign in to drive\n",
        "!git config --global credential.helper store\n",
        "from google.colab import drive\n",
        "if SAVE_GDRIVE:\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"not saving temp files in gdrive\")\n",
        "base_drive_loc = \"/content/drive/MyDrive/Programming\"\n",
        "folder_name = \"eleuther6b\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX6OmyGwKmdK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os.path import join\n",
        "if SAVE_GDRIVE:\n",
        "\n",
        "    outpath = join(os.getcwd(), repo_name)\n",
        "    os.makedirs(outpath, exist_ok=True)\n",
        "    print(f\"created the folder {outpath}.. asking python if it agrees = {os.path.exists(outpath)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU2tmrdyaMmn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2NSgzBVjWvZ"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1A03QtsBjYjS"
      },
      "outputs": [],
      "source": [
        "#@title basics\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def get_workers():\n",
        "\n",
        "    cores = os.cpu_count()\n",
        "\n",
        "    if cores > 2:\n",
        "        return cores - 2 # save 2 for ... things so colab does not crash\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "\n",
        "def print_spacer(n=1):\n",
        "    \"\"\"print_spacer - print a spacer line\"\"\"\n",
        "    print(\"\\n   --------    \" * n)\n",
        "\n",
        "\n",
        "def remove_trailing_punctuation(text: str):\n",
        "    \"\"\"\n",
        "    remove_trailing_punctuation - remove trailing punctuation from a string\n",
        "\n",
        "    Args:\n",
        "        text (str): [string to be cleaned]\n",
        "\n",
        "    Returns:\n",
        "        [str]: [cleaned string]\n",
        "    \"\"\"\n",
        "    return text.strip(\"?!.,;:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODiktIBFt4w"
      },
      "source": [
        "### Converting the model to 8 bits.\n",
        "\n",
        "We convert EleutherAI's GPT-J-6B model to 8 bits using facebook's [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) library. This reduces the model's size from 20Gb down to just 6Gb.\n",
        "\n",
        "Note that we don't convert linear layer biases to 8 bit as they take up less that 1% of the model's weight anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P8Y75B6WDIN-"
      },
      "outputs": [],
      "source": [
        "#@title bitsandbytes custom classes\n",
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sd0Lq-5WNcN"
      },
      "source": [
        "### create blocking functions \n",
        "\n",
        "they convert anything that could be assigned to the model to 8-bit (I think)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOSZ-S1cDRq1"
      },
      "outputs": [],
      "source": [
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gezOGMQ8WsqV"
      },
      "source": [
        "# load pretrained model, config, etc files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthEhmDBSyEm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "gpt_eos = '<|endoftext|>'\n",
        "config = transformers.GPTJConfig.from_pretrained(hf_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(hf_name, \n",
        "                                                       pad_token=gpt_eos,\n",
        "                                                       eos_token=gpt_eos,\n",
        "                                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuW4H6HTS82r"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "gpt = GPTJForCausalLM.from_pretrained(model_8bit,\n",
        "                                      use_auth_token=True,\n",
        "                                      low_cpu_mem_usage=False,\n",
        "                                      gradient_checkpointing=True,\n",
        "                                      use_cache=False, # prevent weird hf trainer warnings popping up\n",
        "                                      )\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRF2QqvzENCr"
      },
      "source": [
        "### Text generation example\n",
        "\n",
        "> testing the base 6B version that was loaded off of hf to validate that it works. This is done on the GPU, so if you are running into memory issues try moving `gpt.to(device)` in a prior cell to after the text has been generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RJd45kFaZ1Ba"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import pprint as pp\n",
        "std_test = \"today I woke up and then\" #@param {type:\"string\"}\n",
        "prompt = tokenizer(std_test, return_tensors='pt')\n",
        "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "ex_min = len(std_test) + 64\n",
        "out = gpt.generate(**prompt, \n",
        "                   min_length=ex_min,\n",
        "                   max_length=ex_min + 64,\n",
        "                   do_sample=True,\n",
        "                    top_k=50,\n",
        "                   top_p=0.9,\n",
        "                   repetition_penalty =1.5,\n",
        "                   length_penalty=1.2,\n",
        "                   no_repeat_ngram_size=2,\n",
        "                   clean_up_tokenization_spaces=True,\n",
        "                   remove_invalid_values=True,\n",
        "                   )\n",
        "\n",
        "example_res = tokenizer.decode(out[0])\n",
        "print(f\"Total generated text is: \\n\")\n",
        "pp.pprint(example_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xY74va0bgBV"
      },
      "source": [
        "# Prep for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V36gOIOfLHvB"
      },
      "outputs": [],
      "source": [
        "#@title load packages for dataset prep\n",
        "#@markdown imports etc are here\n",
        "!pip install -U -q datasets\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from itertools import chain\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sCaCYtpRYin1"
      },
      "outputs": [],
      "source": [
        "#@markdown **Load Train/Test files from URL**\n",
        "#@markdown works for any URL that `wget` would. Otherwise try using `files.upload`\n",
        "#@markdown from the `google.colab` package or click on the side panel\n",
        "train_link = \"https://www.dropbox.com/s/olnx438omur7j72/wow-train.txt.txt?dl=1\" #@param {type:\"string\"}\n",
        "test_link = \"https://www.dropbox.com/s/t2hhawpsiocypyt/ScriptParse-wow-train-kilt_4.txt?dl=1\" #@param {type:\"string\"}\n",
        "\n",
        "vm_wd = os.getcwd()\n",
        "train_path = join(vm_wd, \"train_dataset.txt\")\n",
        "request.urlretrieve(train_link, train_path)\n",
        "\n",
        "# test file\n",
        "test_path = join(vm_wd, \"test_dataset.txt\")\n",
        "request.urlretrieve(test_link, test_path)\n",
        "\n",
        "print(f\"DL complete. Path to train data file is:\\n{train_path}\\nand path to test data is:\\n{test_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz0YXU0_YmnR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown **initialize `datasets` object** \n",
        "import warnings\n",
        "#@markdown a lot of the next couple sections are borrowed from [here](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py)\n",
        "#@markdown because they decided to change the APIs. Also, [this page](https://huggingface.co/docs/datasets/access.html)\n",
        "#@markdown is a decent place to start for datasets / API. Additionally, [this doc page](https://huggingface.co/docs/datasets/loading.html)\n",
        "#@markdown goes slightly more in depth because datasets are confusing and complicated at first.\n",
        "\n",
        "process_style = \"line_by_line\" #@param [\"line_by_line\", \"merge\"]\n",
        "train_keep = 92 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "val_keep = 97 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "data_files = {}\n",
        "if train_path is not None:\n",
        "    data_files[\"train\"] = train_path\n",
        "    extension = train_path.split(\".\")[-1]\n",
        "if test_path is not None:\n",
        "    data_files[\"validation\"] = test_path\n",
        "    extension = test_path.split(\".\")[-1]\n",
        "if extension == \"txt\":\n",
        "    extension = \"text\"\n",
        "\n",
        "# original\n",
        "# raw_datasets = load_dataset(extension, data_files=data_files,)\n",
        "# tokenizer defined earlier\n",
        "rds_train = load_dataset(extension, \n",
        "                         data_files=data_files[\"train\"],\n",
        "                         split=f\"train[:-{train_keep}%]\")\n",
        "rds_val = load_dataset(extension, \n",
        "                       data_files=data_files[\"validation\"],\n",
        "                         split=f\"train[:-{val_keep}%]\")\n",
        "\n",
        "\n",
        "# raw_datasets = datasets.DatasetDict(rds_train, rds_val)\n",
        "raw_datasets = datasets.DatasetDict({\"train\":rds_train,\"validation\":rds_val})\n",
        "    # \"train\":rds_train,\n",
        "    # \"validation\":rds_val,\n",
        "column_names = raw_datasets[\"train\"].column_names\n",
        "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "max_seq_length = tokenizer.model_max_length\n",
        "if max_seq_length > 1024 and max_seq_length != 2048:\n",
        "    warnings.warn(\n",
        "        f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
        "        \"Picking 1024 instead. You can change that default value by editing this cell wow!!\"\n",
        "    )\n",
        "    max_seq_length = 1024\n",
        "\n",
        "pp.pprint(raw_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XVwy3j-vc8xc"
      },
      "outputs": [],
      "source": [
        "#@title where the ~~magic~~ tokenization happens\n",
        "if process_style == \"line_by_line\":\n",
        "    # When using line_by_line, we just tokenize each nonempty line.\n",
        "    padding = \"max_length\" # if you do want padding then change this to False\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # Remove empty lines\n",
        "        examples[text_column_name] = [\n",
        "            line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n",
        "        ]\n",
        "        return tokenizer(\n",
        "            examples[text_column_name],\n",
        "            padding=padding,\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "            # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
        "            # receives the `special_tokens_mask`.\n",
        "            return_special_tokens_mask=True,\n",
        "        )\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        num_proc=get_workers(),\n",
        "        remove_columns=[text_column_name],\n",
        "        desc=\"Running tokenizer on dataset line_by_line\",\n",
        "    )\n",
        "else:\n",
        "    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
        "    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
        "    # efficient when it receives the `special_tokens_mask`.\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[text_column_name], padding=padding,\n",
        "                         return_special_tokens_mask=True)\n",
        "\n",
        "    tokenized_datasets = raw_datasets.map(\n",
        "                    tokenize_function,\n",
        "                    batched=True,\n",
        "                    num_proc=get_workers(),\n",
        "                    remove_columns=column_names,\n",
        "                    desc=\"Running tokenizer on every text in dataset\",\n",
        "                )\n",
        "\n",
        "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
        "    # max_seq_length.\n",
        "    def group_texts(examples):\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "        if total_length >= max_seq_length:\n",
        "            total_length = (total_length // max_seq_length) * max_seq_length\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        return result\n",
        "\n",
        "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
        "    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
        "    # might be slower to preprocess.\n",
        "    #\n",
        "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
        "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
        "\n",
        "    tokenized_datasets = tokenized_datasets.map(\n",
        "        group_texts,\n",
        "        batched=True,\n",
        "        num_proc=get_workers(),\n",
        "        desc=f\"Grouping texts in chunks of {max_seq_length}\",\n",
        "    )\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "print(f\"created train and evaluation with {train_dataset.num_rows} and {eval_dataset.num_rows} rows respectively\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mIBX0x2imWvb"
      },
      "outputs": [],
      "source": [
        "#@markdown **Create Data collator**\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "#@markdown no random masking needed because this is text gen. Docs [here](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForLanguageModeling)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "                    tokenizer=tokenizer,\n",
        "                    mlm=False,\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdLQHOuEU7h"
      },
      "source": [
        "# LoRA fine-tuning example\n",
        "Here we demonstrate how to fine-tune the proposed model using low-rank adapters [(Hu et al, 2021)](https://arxiv.org/abs/2106.09685) and [8-bit Adam](https://arxiv.org/abs/2110.02861). We also use [dataset streaming API](https://huggingface.co/docs/datasets/dataset_streaming.html) to avoid downloading the large dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ctu4Q5aq-g"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "def add_adapters(model, adapter_dim=16):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, FrozenBNBLinear):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "        elif isinstance(module, FrozenBNBEmbedding):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "\n",
        "add_adapters(gpt)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DDBP8RPUJ2Jn"
      },
      "outputs": [],
      "source": [
        "#@title training args\n",
        "\n",
        "#@markdown > here is where you enter / select parameters for variables that are used\n",
        "#@markdown > throughout the rest of the notebook.\n",
        "\n",
        "#@markdown - the docs on the [trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) are very lovely and short. You can also reference [the main training overview](https://huggingface.co/docs/transformers/training) from earlier\n",
        "#@markdown - if you have issues maintaing a stable colab session, try changing the `save_strategy ` to `steps` and choose a reasonable number (based on the specific situation)\n",
        "\n",
        "START_LR = 1e-4 #@param {type:'number'}\n",
        "GC_STEPS =  64#@param {type:'integer'}\n",
        "MAX_GRAD_NORM = 0.5 #@param {type:'number'}\n",
        "USE_FP16 = True #@param {type:\"boolean\"}\n",
        "HUB_PUSH = True #@param {type:\"boolean\"}\n",
        "BATCH_SIZE =  2#@param {type:\"integer\"}\n",
        "\n",
        "full_out_name = f\"{hf_name.split('/')[-1]}-{model_name_header}_DS-{dataset}_Ep-{N_EPOCHS}_Bs-{BATCH_SIZE}\"\n",
        "# model.push_to_hub(full_out_name, auth)\n",
        "full_out_name = full_out_name.replace(\".\", \"pt\")\n",
        "print(f\"\\nmodel will be stored on HF as:\\n {full_out_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fr_z8idK9h1F"
      },
      "outputs": [],
      "source": [
        "#@title Define Metrics\n",
        "\n",
        "#@markdown <font color=\"orange\"> please note that at present no evaluation is completed in \n",
        "#@markdown by default - it provides to add a second \"layer\" of GPU compute that seems to be too much\n",
        "#@markdown so that will be computed separately.\n",
        "\n",
        "#@markdown Taken from [this](https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1)\n",
        "#@markdown great article on medium:\n",
        "\n",
        "#@markdown > BLEU and Rouge are the most popular evaluation metrics that are used to compare models in the NLG domain. BLEU is a precision focused metric that calculates n-gram overlap of the reference and generated texts. This n-gram overlap means the evaluation scheme is word-position independent apart from n-grams’ term associations. One thing to note in BLEU — there is a brevity penalty i.e. a penalty applied when the generated text is too small compared to the target text.\n",
        "\n",
        "#@markdown > Rouge: Recall Oriented Understudy for Gisting Evaluation  It is very similar to the BLEU definition, the difference being that Rouge is recall focused whereas BLEU was precision focused. There are 3 types of Rouge: n-rouge, the most common rouge type which means n-gram overlap. eg. (2-rouge, 1-rouge for 2-grams and 1-gram respectively). Second is l-rouge which checks for Longest Common Subsequence instead of n-gram overlap. The third is s-rouge which focuses on skip grams. Standard implementations of these can be found in most ML libraries, n-rouge is most commonly used.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown reference [here]( https://huggingface.co/docs/transformers/training) from hf docs\n",
        "import numpy as np\n",
        "from datasets import load_metric, list_metrics\n",
        "MY_METRIC = \"bleu\" #@param {type:'string'}\n",
        "EVAL_WHILE_TRAIN = False #@param {type:'boolean'}\n",
        "\n",
        "metric = load_metric(MY_METRIC)# accuracy or check list_metrics()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "if EVAL_WHILE_TRAIN: print(f\"will compute the {MY_METRIC} metric while training, expect higher mem usage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIvtbOL0rZcT"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    output_dir=join(outpath, \"checkpoints\") if SAVE_GDRIVE else './checkpoints', \n",
        "    save_total_limit=1,\n",
        "    logging_dir='/content/logs',\n",
        "    num_train_epochs=1, \n",
        "    # TODO figure out and fix issue. no eval mid-train for now\n",
        "    evaluation_strategy='epoch' if EVAL_WHILE_TRAIN else 'no', \n",
        "    save_strategy='steps', # switched to every N steps because it did not seem to work otherwise\n",
        "    save_steps=30,\n",
        "    logging_steps=10,\n",
        "\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    per_device_train_batch_size=BATCH_SIZE, \n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GC_STEPS, \n",
        "    eval_accumulation_steps= 0 if GC_STEPS == 0 else 4,\n",
        "    gradient_checkpointing=False if GC_STEPS == 0 else True,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    lr_scheduler_type  = \"cosine_with_restarts\",\n",
        "    learning_rate=START_LR,\n",
        "    warmup_ratio=0.05,\n",
        "    weight_decay=0.01,\n",
        "    # bf16=True,  # only switch to BF16 if you have A100 or better (somehow??)\n",
        "    # bf16_full_eval=True,\n",
        "    fp16_full_eval=USE_FP16, # if metrics are crucial set to False\n",
        "    fp16=USE_FP16,\n",
        "    fp16_opt_level='O1',\n",
        "    # deepspeed=ds_config, # use deepspeed.\n",
        "    push_to_hub=HUB_PUSH,\n",
        "    hub_model_id=full_out_name,\n",
        "    hub_strategy='checkpoint',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Cqtaw5Ucqw9c"
      },
      "outputs": [],
      "source": [
        "#@title define Schedule and Optimizer\n",
        "#@markdown because we are using the `trainer()` api instead of boilerplate code, need to\n",
        "#@markdown define these. See [here](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
        "#@markdown for docs on learning rate scheduling\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "gpt.resize_token_embeddings(len(tokenizer))\n",
        "gpt.gradient_checkpointing_enable()\n",
        "\n",
        "#@markdown `LR_GAMMA` is the Multiplicative factor of learning rate decay / epoch.\n",
        "LR_GAMMA = 0.66 #@param {type:'number'}\n",
        "optimizer = Adam8bit(gpt.parameters(), lr=START_LR) \n",
        "scheduler = lr_scheduler.ExponentialLR(optimizer, \n",
        "                                       gamma=LR_GAMMA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sfSYnL7tpY0m"
      },
      "outputs": [],
      "source": [
        " #@title Init Trainer\n",
        " #@markdown basically, now this just puts all the pieces together.\n",
        "from datasets import load_metric, list_metrics\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    optimizers=(optimizer,scheduler),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "G8lGzRdgZRGc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If7qFm2uwwHV"
      },
      "source": [
        "## Run Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndCyeBsGwyrw"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "for ep in range(N_EPOCHS):\n",
        "    gc.collect()\n",
        "    trainer.train()\n",
        "    trainer.push_to_hub()\n",
        "    tokenizer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXqTv8kqxpJ"
      },
      "source": [
        "## save & convert\n",
        "\n",
        "After training is done you can save the model by calling `save_model()`. This will save the trained model to our `output_dir` from our `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl8s5qAseS6Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown prep work - imports, create directories for conversion\n",
        "import os\n",
        "from os.path import join\n",
        "from google.colab import files \n",
        "import gc\n",
        "fin_zero = join(os.getcwd(), \"final_zero_weights\")\n",
        "os.makedirs(fin_zero, exist_ok=True )\n",
        "\n",
        "fin_loc = join(os.getcwd(), \"final_model\")\n",
        "os.makedirs(fin_loc, exist_ok=True )\n",
        "\n",
        "print(f\"final model file will be saved to {fin_loc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5quyGeMNdjE"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(output_dir=fin_loc) # save to one directory with zero weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R9OFizppKuA"
      },
      "outputs": [],
      "source": [
        "del trainer\n",
        "del gpt\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pJ7hnO-7ZPEb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUUyrzEhw-Be"
      },
      "source": [
        "# Push latest to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRa9mAYVrYZK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown re-load tokenizer from the original model if something happened and it \n",
        "#@markdown does not exist\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "if \"_tokenizer\" not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_name, use_fast=False,\n",
        "                                            max_length=2048,\n",
        "                                            model_max_length=2048,\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfCZ9uxlynx_"
      },
      "outputs": [],
      "source": [
        "# Push the tokenizer to your namespace with the name full_out_name with no local clone.\n",
        "tokenizer.push_to_hub(full_out_name,\n",
        "                       use_auth_token=True,\n",
        "                    use_temp_dir=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XV2H0bfXsZu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Reload the \"Converted\" Model\n",
        "%%capture\n",
        "from os.path import join\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "finetuned_model =GPTJForCausalLM.from_pretrained(fin_loc,\n",
        "                                      low_cpu_mem_usage=True,\n",
        "                                      use_cache=False, # prevent weird hf trainer warnings popping up\n",
        "                                      )\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "finetuned_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qUygJt9yfvT"
      },
      "outputs": [],
      "source": [
        "finetuned_model.push_to_hub(full_out_name,\n",
        "                            use_auth_token=True,\n",
        "                            use_temp_dir=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtCcq6Jc86-A"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk2g7SblfMVZ"
      },
      "source": [
        "# Generate text with trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeV4O82xlbEZ"
      },
      "source": [
        "## add prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCdyTvnf5XOK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "finetuned_model.to('cuda')\n",
        "\n",
        "my_chatbot = pipeline('text-generation', model=finetuned_model, \n",
        "                      tokenizer=tokenizer,\n",
        "                      device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nqQWB_r0ndUd"
      },
      "outputs": [],
      "source": [
        "#@title define speaker and responder\n",
        "#@markdown for testing the models this should not need to be changed. \n",
        "#@markdown if testing a model related to [ai-msgbot](https://github.com/pszemraj/ai-msgbot)\n",
        "#@markdown trained on data that **was not** using the entries below, update as needed.\n",
        "speaker = \"person alpha\" #@param {type:\"string\"}\n",
        "responder = \"person beta\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPaBftn6qEo7"
      },
      "source": [
        "## define prompt messages\n",
        "\n",
        "the reason `f\"{responder}:\\n\"` is added at the end of each prompt is to force the text-gen model to actually _respond_ to the prompt as opposed to adding on to it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "           \"hi! how are you doing?\",\n",
        "           \"what should I bring to the party?\",\n",
        "           \"do you like memes?\",\n",
        "           \"can we go on a date together this weekend?\",\n",
        "           \"what's up homie?\",\n",
        "           \"do you know how can I make friends here?\",\n",
        "           \"so what do you like to do for fun?\",\n",
        "           \"what is your favorite brand of cereal?\",\n",
        "           \"what is the meaning of existence?\",\n",
        "]"
      ],
      "metadata": {
        "id": "BLUXVUspuYBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdGJdXw3dUr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title encode prompts\n",
        "#@markdown defines `encode_prompt` as a helper\n",
        "\n",
        "\n",
        "def encode_prompt(prompt:str, spkr=None, rspndr=None):\n",
        "    line1 = f\"{spkr}:\\n\" if spkr is not None else \"\"\n",
        "    line2 = f\"{prompt}\\n\\n\"\n",
        "    line3 = f\"{rspndr}:\\n\" if rspndr is not None else \"\"\n",
        "\n",
        "    return [line1, line2, line3]\n",
        "\n",
        "\n",
        "encoded_prompts = [encode_prompt(p, speaker, responder) for p in prompts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglT-9cB6Aay"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wsAeVa6HzuZ2"
      },
      "outputs": [],
      "source": [
        "#@markdown set amount of text to generate (higher # = longer RT)\n",
        "resp_len =  128#@param {type:\"integer\"}\n",
        "resp_temp =  0.72#@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNJfogki47Au"
      },
      "outputs": [],
      "source": [
        "# note that responses output the prompt as part of the output (and that counts \n",
        "# for part of the max length reqs)\n",
        "for i, question in enumerate(encoded_prompts):\n",
        "    this_prompt = \"\".join(question)\n",
        "    ex_min = len(this_prompt) + resp_len\n",
        "    result = my_chatbot(\n",
        "                        this_prompt,\n",
        "                        min_length=ex_min,\n",
        "                        max_length=ex_min + resp_len,\n",
        "                        do_sample=True,\n",
        "                        top_k=35,\n",
        "                        top_p=0.90,\n",
        "                        temperature=resp_temp,\n",
        "                        # repetition_penalty =1.5,\n",
        "                        # length_penalty=1.2,\n",
        "                        no_repeat_ngram_size=4,\n",
        "                        clean_up_tokenization_spaces=True,\n",
        "                    )\n",
        "    \n",
        "    print(f\"==========Testing Prompt-ID #{i} ==========\")\n",
        "    print(f\"PROMPT TEXT:\\n{''.join(question)}\")\n",
        "    print(\"----------FULL GENERATED TEXT:\")\n",
        "    print(result[0]['generated_text'])\n",
        "    print(\"\\n\" * 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__ZGank_Dr4k"
      },
      "source": [
        "# Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9PP84fDu3M"
      },
      "outputs": [],
      "source": [
        "metadata = training_args.to_sanitized_dict()\n",
        "metadata[\"configs_src\"] = hf_name\n",
        "metadata[\"model_src\"] = model_8bit\n",
        "metadata[\"train_tag\"] = model_name_header\n",
        "metadata[\"data_tag\"] = dataset\n",
        "metadata[\"LR_scheduler_gamma\"] = LR_GAMMA\n",
        "\n",
        "pp.pprint(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlh-pkTNTIyX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "metadata_path = f\"{model_name_header}_training_metadata.json\"\n",
        "with open(metadata_path, \"w\") as write_file:\n",
        "    json.dump(metadata, write_file)\n",
        "\n",
        "files.download(metadata_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYEEFAL9CO5N"
      },
      "source": [
        "# ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aipQNQwXCRN8"
      },
      "source": [
        "- train with [story-cloze](https://huggingface.co/datasets/story_cloze) dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "finetune-gpt-j-6B-8bit.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}